<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Linux基本命令]]></title>
    <url>%2F2019%2F01%2F14%2FLinux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[摘要：Linux基础 一、基本命令1.1 相对路径：12345~ # home文件夹cd .. # 返回上级目录cd ../../ # 往上返回两次cd - # 返回上级目录cd ~ # 返回home文件夹 1.2 绝对路径：1cd /home/ubuntu/Documents 1.3 浏览文件夹下内容：12345678910# 查看文件夹下的文件及文件夹(ls: list)ls# 显示文件夹下的文件及文件夹详细信息(-l: long)ls -l# 显示文件夹下的文件及文件夹详细信息，显示文件大小单位(-lh: long human)ls -lh# 显示全部（包含隐藏）文件夹下的文件及文件夹详细信息(-a: all)ls -a# 显示ls其他功能ls --help 1.4 创建文件：123456# 进入文件夹，在该文件夹内创建touch file1# 创建多个文件touch file2 file3 file4# 创建文件夹mkdir folder 1.5 复制文件：1234567891011# 复制file1到filecopy，但是不可检测filecopy是否已经存在cp file1 file1copy# 复制file1到filecopy，检测filecopy是否已经存在（-i: interactive）cp -i file1 filecopy# 复制文件到另一个文件夹中cp file1 folder/cp file1 file2 folder/# 复制一个文件中的所有文件到另一个文件夹中(-R 递归)cp -R folder1/ folder2/# 复制所有以file开头的文件到另一个文件夹中cp file* folder2/ 1.6 移动文件：1234# 移动文件到文件夹mv file1 folder1/# 重命名mv file2 file2rename 1.7 删除文件：12345678# 只能删除空文件夹rmdir folder3# 删除文件夹及里面内容(-r 递归)rm -r folder3# 删除文件(每一个都询问确认)rm -i file1 file2 file3# 删除文件(超过3个才会询问确认)rm -I file1 file2 1.8 修改文件：12345678910111213# 创建（修改）文件nano t.py# crtl + x 保存退出![文件权限](F:\blog\hexo\source\_posts\Linux基本命令\文件权限.png)![文件权限](F:\blog\hexo\source\_posts\Linux基本命令\文件权限.png)# 运行Python脚本python3 t.py# 显示在屏幕上cat t.py# 复制到另一个文件cat t.py &gt; t1.py# 把多个文件合并到新的文件中cat t.py t1.py &gt; t2.py# 把文件追加到已有文件之后cat t3.py &gt;&gt; t2.py 1.9 显示当前路径1pwd 二、文件权限 Type: 很多种 (最常见的是 - 为文件, d 为文件夹, 其他的还有l, n … 这种东西, 真正自己遇到了, 网上再搜就好, 一次性说太多记不住的). User: 后面跟着的三个空是使用 User 的身份能对这个做什么处理 (r 能读; w 能写; x能执行; - 不能完成某个操作). Group: 一个 Group 里可能有一个或多个 user, 这些权限的样式和 User 一样. Others: 除了 User 和 Group 以外人的权限. 修改权限： 1234567891011# 形式如下：chmod [谁][怎么修改] [哪个文件]# 修改t1.py的user加上r(可读)chmod u+r t1.pyls -l t1.py # 显示 t1.py 的权限# t1.py的u,g,o 都加上w(可写)chmod a+w t1.pyls -l t1.py # 显示 t1.py 的权限# t1.py的u,g 都加上r,w(可读、写)chmod ug+rw t1.pyls -l t1.py # 显示 t1.py 的权限 [谁] u: 对于 User 修改 g: 对于 Group 修改 o: 对于 Others 修改 a: (all) 对于所有人修改 [怎么修改] +, -, =: 作用的形式, 加上, 减掉, 等于某些权限 r, w, x 或者多个权限一起, 比如 rx [哪个文件] 施加操作的文件, 可以为多个 把 .py 文件可执行 1chmod u+w t1.py t1.py 文件内加上一句话： 12# 指定用什么打开#！/home/xing/anaconda3/bin/python3 三、VMware安装Ubuntu3.1 安装拼音输入法 3.2 安装VMware Tools，实现窗口的适应大小，Windows和虚拟机实现复制粘贴。 3.3 Ubuntu系统安装之后，一般没有pip2和pip3，需要手动安装；之后会同时出现pip（不知道什么意思）, pip2, pip3。 1234# 安装pip2sudo apt-get install python-pip# 安装pip3sudo apt-get install python-pip 四、Anaconda环境管理参考教程：https://blog.csdn.net/yimingsilence/article/details/79388205 1. 管理conda1234# 查看版本：conda --version# 升级版本：conda update conda 2. 创建并激活一个环境1234567# 创建环境：# 环境名字为：xsc 同时安装python3.6 numpy pandas 包conda create --name xsc python=3.6 numpy pandas # 激活环境Linux，OS X: source activate snowflakesWindows：activate snowflake`# 如果我们没有指定安装python的版本，conda会安装我们最初安装conda时所装的那个版本的python。 3. 列出所有的环境123456789101112131415# 列出所有的环境conda info --envs或者：conda env list# 确认当前环境conda info -envis# 复制一个环境conda create -n flowers --clone xsc# 删除一个环境conda remove -n flowers --all或conda env remove -n py3# 注销环境·Linux，OS X：source deactivate·Windows：deactivate 4. 管理Python12# 检查哪个版本的python可以被安装conda search --full --name python 5. 管理包123456# 查看该环境中包和其版本的列表conda list# 查找一个包conda search beautifulsoup4# 必须告诉conda你要安装环境的名字（-n xsc）否则它将会被安装到当前环境中conda install --name xsc beautifulsoup4 6. Anaconda.org安装一个包如果一个包不能使用conda安装，我们接下来将在Anaconda.org网站查找。Anaconda.org向公开和私有包仓库提供包管理服务。Anaconda.org是一个连续分析产品。提示：你在Anaconda.org下载东西的时候不强制要求注册。为了从Anaconda.org下载到当前的环境中，我们需要通过指定Anaconda.org为一个特定通道，通过输入这个包的完整路径来实现。在浏览器中，去 http://anaconda.org 网站。我们查找一个叫“bottleneck”的包，所以在左上角的叫“Search Anaconda Cloud”搜索框中输入“bottleneck”并点击search按钮。Anaconda.org上会有超过一打的bottleneck包的版本可用，但是我们想要那个被下载最频繁的版本。所以你可以通过下载量来排序，通过点击Download栏。 1conda install --channel https：//conda .anaconda.ort/pandas bottleneck 7. pip命令来安装包对于那些无法通过conda安装或者从Anaconda.org获得的包，我们通常可以用pip（“pip install packages”的简称）来安装包。提示： pip只是一个包管理器，所以它不能为你管理环境。pip甚至不能升级python，因为它不像conda一样把python当做包来处理。但是它可以安装一些conda安装不了的包，和vice versa（此处不会翻译）。pip和conda都集成在Anaconda或miniconda里边。 1234·Linux，OS X： source activate bunnies·Windows：activate bunnies# 所有平台：pip install see 8. 移除包、环境、或者conda12345678910# 在 xsc 环境中移除 ioproconda remove -n xsc iopro# 移除环境# 不再需要 xsc 环境了，所以输入以下命令：conda remove -n xsc --all# 删除conda# Linux，OS X：移除Anaconda 或 Miniconda 安装文件夹rm -rf ~/miniconda OR rm -rf ~/anaconda# Windows：去控制面板，点击“添加或删除程序”，选择“Python2.7（Anaconda）”或“Python2.7（Miniconda）”并点击删除程序。 9. 查看 CUDA 版本1234# 查看 CUDA 版本cat /usr/local/cuda/version.txt# 查看 CUDNN 版本cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2 10. 查看GPU信息Linux查看显卡信息： 1lspci | grep -i vga 使用nvidia GPU可以： 1lspci | grep -i nvidia 前边的序号 “0b:00.0”是显卡的代号(这里是用的虚拟机); 查看指定显卡的详细信息用以下指令： 1lspci -v -s 0b:00.0 查看Nvidia显卡信息及使用情况: 1nvidia-smi 这是服务器上 GeForce GTX 1080 Ti 的信息。上面的表格中：第一栏的Fan：N/A是风扇转速，从0到100%之间变动，这个速度是计算机期望的风扇转速，实际情况下如果风扇堵转，可能打不到显示的转速。有的设备不会返回转速，因为它不依赖风扇冷却而是通过其他外设保持低温（比如我们实验室的服务器是常年放在空调房间里的）。第二栏的Temp：是温度，单位摄氏度。第三栏的Perf：是性能状态，从P0到P12，P0表示最大性能，P12表示状态最小性能。第四栏下方的Pwr：是能耗，上方的Persistence-M：是持续模式的状态，持续模式虽然耗能大，但是在新的GPU应用启动时，花费的时间更少，这里显示的是off的状态。第五栏的Bus-Id是涉及GPU总线的东西，domain:bus:device.function第六栏的Disp.A是Display Active，表示GPU的显示是否初始化。第五第六栏下方的Memory Usage是显存使用率。第七栏是浮动的GPU利用率。第八栏上方是关于ECC的东西。第八栏下方Compute M是计算模式。下面一张表示每个进程占用的显存使用率。 显存占用和GPU占用是两个不一样的东西，显卡是由GPU和显存等组成的，显存和GPU的关系有点类似于内存和CPU的关系。我跑caffe代码的时候显存占得少，GPU占得多，师弟跑TensorFlow代码的时候，显存占得多，GPU占得少。]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Python易错总结]]></title>
    <url>%2F2019%2F01%2F11%2FPython%E6%98%93%E9%94%99%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[摘要：易错点 1. range(),arange()函数区别Python 中的range,以及numpy包中的arange函数 1.1 range()函数： 函数说明： range(start, stop[, step]) -&gt; range object，根据start与stop指定的范围以及step设定的步长，生成一个序列，步长不能为float,，默认为1。 参数含义：start:计数从start开始。默认是从0开始。例如range（5）等价于range（0， 5）; end:技术到end结束，但不包括end.例如：range（0， 5） 是[0, 1, 2, 3, 4]没有5 scan：每次跳跃的间距，默认为1。例如：range（0， 5） 等价于 range(0, 5, 1) range返回一个list 12345678&gt;&gt;&gt; range(0,5) #生成一个range object,而不是[0,1,2,3,4] range(0, 5) &gt;&gt;&gt; c = [i for i in range(0,5)] #从0 开始到4，不包括5，默认的间隔为1&gt;&gt;&gt; c[0, 1, 2, 3, 4]&gt;&gt;&gt; c = [i for i in range(0,5,2)] #间隔设为2&gt;&gt;&gt; c[0, 2, 4] 1.2 arrange()函数 函数说明：arange([start,] stop[, step,], dtype=None)根据start与stop指定的范围以及step设定的步长，步长可以是float，生成一个 ndarray。 12345678910&gt;&gt;&gt; np.arange(3)array([0, 1, 2])&gt;&gt;&gt; np.arange(3.0)array([ 0., 1., 2.])&gt;&gt;&gt; np.arange(3,7)array([3, 4, 5, 6])&gt;&gt;&gt; np.arange(3,7,2)array([3, 5])&gt;&gt;&gt; arange(0,1,0.1)array([ 0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]) #2. list、array、matrix的区别 2.1 numpyNumPy的主要对象是同种元素的多维数组。这是一个所有的元素都是一种类型、通过一个正整数元组索引的元素表格(通常是元素是数字)。在NumPy中维度(dimensions)叫做轴(axes)，轴的个数叫做秩(rank，但是和线性代数中的秩不是一样的，在用python求线代中的秩中，我们用numpy包中的linalg.matrix_rank方法计算矩阵的秩。 列表属于python的三种基本集合类型之一, 其他两种是元组(tuple)和字典(dict). tuple和list区别主要在于是不是mutable（可变）的. list和java里的数组不同之处在于, python的list可以包含任意类型的对象, 一个list里可以包含int, string或者其他任何对象, 另外list是可变长度的(list有append, extend和pop等方法). 所以, python内建的所谓”列表”其实是功能很强大的数组, 类比一下可以说它对应于java里面的ArrayList . 2.2 ndarray多维数组 ndarray是numpy的基石, 其实它更像一个java里面的标准数组: 所有元素有一个相同数据类型(dtype), 不过大小不是固定的. ndarray对于大计算量的性能非常好, 所以list要做运算的时候一定要先转为np.array(a_list)).数组转化为列表：a.tolist() ndarray带有一些非常实用的函数, 列举几个常用的: sum, cumsum, argmax, reshape, T, … ndarray有fancy indexing, 非常实用, 比如: a[a&gt;3] 返回数组里大于3的元素 ndarray之间的乘法: 如果用乘法运算符*的话, 返回的是每个位置元素相乘(类似matlab里面的.*), 想要矩阵相乘需要用dot(). 常见矩阵的生成: ones, zeros, eye, diag, … 2.3 matrix矩阵matrix是ndarray的子类, 所以前面ndarray那些优点都保留了. 同时, matrix全部都是二维的, 并且加入了一些更符合直觉的函数, 比如对于matrix对象而言, 乘号运算符得到的是矩阵乘法的结果. 另外mat.I就是逆矩阵… 不过应用最多的还是ndarray类型. 3. multiply、dot、 * 和matmul3.1. np.multiply()函数函数作用： 数组和矩阵对应位置相乘，输出与相乘数组/矩阵的大小一致 3.2 np.dot()函数函数作用： 对于秩为1的数组，执行对应位置相乘，然后再相加； 对于秩不为1的二维数组或者矩阵，执行矩阵乘法运算；超过二维的可以参考numpy库介绍。 3.3 星号（*）乘法运算函数作用： 对数组执行对应位置相乘 对矩阵执行矩阵乘法运算 参考链接 3.4 matmul()两个数组的矩阵乘积。不允许使用标量乘法，*可以使用 matmul与dot基本一样，但有两个方面的不同： 不允许使用标量进行乘法运算。 矩阵堆栈一起广播，就好像矩阵是元素一样。 4. randn()与rand()的区别numpy中有一些常用的用来产生随机数的函数，randn()和rand()就属于这其中。 numpy.random.randn(d0, d1, …, dn)是从标准正态分布中返回一个或多个样本值。 numpy.random.rand(d0, d1, …, dn)的随机样本位于[0, 1)中。 12345678910111213import numpy as np arr1 = np.random.randn(2,4)print(arr1)print('*****************************')arr2 = np.random.rand(2,4)print(arr2)[[-1.03021018 0.5197033 0.52117459 -0.70102661] [ 0.98268569 1.21940697 -1.095241 -0.38161758]]**********************************[[ 0.19947349 0.05282713 0.56704222 0.45479972] [ 0.28827103 0.1643551 0.30486786 0.56386943]] 5. filter()1filter(function, iterable) 用于过滤序列，过滤掉不符合条件的元素，返回一个迭代器对象，如果要转换为列表，可以使用 list() 来转换。 接收两个参数，第一个为函数，第二个为序列，序列的每个元素作为参数传递给函数进行判，然后返回 True 或 False，最后将返回 True 的元素放到新列表中。 12345678def is_odd(n): return n % 2 == 1 tmplist = filter(is_odd, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10])newlist = list(tmplist)print(newlist)[1, 3, 5, 7, 9] 6. lambda 表达式1lambda argument_list: expression 参数列表由逗号分隔的参数列表组成，表达式是使用这些参数的算术表达式。您可以将函数分配给变量以为其指定名称。 123&gt;&gt;&gt; sum = lambda x，y：x + y&gt;&gt;&gt;sum(3,4）7 7. zip()zip() 函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的对象，这样做的好处是节约了不少的内存。 可以使用 list() 转换来输出列表。 如果各个迭代器的元素个数不一致，则返回列表长度与最短的对象相同，利用 * 号操作符，可以将元组解压为列表。 1zip([iterable, ...]) iterabl – 一个或多个迭代器; 返回一个对象。 12345678910111213141516&gt;&gt;&gt;a = [1,2,3]&gt;&gt;&gt; b = [4,5,6]&gt;&gt;&gt; c = [4,5,6,7,8]&gt;&gt;&gt; zipped = zip(a,b) # 返回一个对象&gt;&gt;&gt; zipped&lt;zip object at 0x103abc288&gt;&gt;&gt;&gt; list(zipped) # list() 转换为列表[(1, 4), (2, 5), (3, 6)]&gt;&gt;&gt; list(zip(a,c)) # 元素个数与最短的列表一致[(1, 4), (2, 5), (3, 6)] &gt;&gt;&gt; a1, a2 = zip(*zip(a,b)) # 与zip相反，zip(*) 可理解为解压，返回二维矩阵式&gt;&gt;&gt; list(a1)[1, 2, 3]&gt;&gt;&gt; list(a2)[4, 5, 6] 8. set()set() 函数创建一个无序不重复元素集，可进行关系测试，删除重复数据，还可以计算交集、差集、并集等。 1class set([iterable]) iterable – 可迭代对象对象 12345678910&gt;&gt;&gt;x = set('runoob')&gt;&gt;&gt; y = set('google')&gt;&gt;&gt; x, y(set(['b', 'r', 'u', 'o', 'n']), set(['e', 'o', 'g', 'l'])) # 重复的被删除&gt;&gt;&gt; x &amp; y # 交集set(['o'])&gt;&gt;&gt; x | y # 并集set(['b', 'e', 'g', 'l', 'o', 'n', 'r', 'u'])&gt;&gt;&gt; x - y # 差集set(['r', 'b', 'u', 'n']) 9. format格式化字符串的函数 str.format()，它增强了字符串格式化的功能。 基本语法是通过 {} 和 : 来代替以前的 % 。 format 函数可以接受不限个参数，位置可以不按顺序。 1234567891011121314151617181920212223&gt;&gt;&gt;"&#123;&#125; &#123;&#125;".format("hello", "world") # 不设置指定位置，按默认顺序'hello world' &gt;&gt;&gt; "&#123;0&#125; &#123;1&#125;".format("hello", "world") # 设置指定位置'hello world' &gt;&gt;&gt; "&#123;1&#125; &#123;0&#125; &#123;1&#125;".format("hello", "world") # 设置指定位置'world hello world'print("网站名：&#123;name&#125;, 地址 &#123;url&#125;".format(name="菜鸟教程", url="www.runoob.com")) # 通过字典设置参数site = &#123;"name": "菜鸟教程", "url": "www.runoob.com"&#125;print("网站名：&#123;name&#125;, 地址 &#123;url&#125;".format(**site)) # 通过列表索引设置参数my_list = ['菜鸟教程', 'www.runoob.com']print("网站名：&#123;0[0]&#125;, 地址 &#123;0[1]&#125;".format(my_list)) # "0" 是必须的# 通过元组设置参数site = ("name", "菜鸟教程", "url", "www.runoob.com")print('&#123;&#125;-&#123;&#125;-&#123;&#125;-&#123;&#125;'.format(*site))&gt;&gt;&gt; name-菜鸟教程-url-www.runoob.com]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CRF++ 分词实验]]></title>
    <url>%2F2018%2F12%2F14%2FCRF%E5%88%86%E8%AF%8D%E5%AE%9E%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[摘要：CRF++ 分词实验 1. 安装在Windows下的安装很简单，其实严格来讲不能说是安装。我们解压我们下载的压缩包文件到某一个目录下面。你可能会得到如下所示的文件，（版本不同，可能会有所不同。） doc文件夹：就是官方主页的内容 example文件夹：有四个任务的训练数据（test.data）、测试数据(train.data)和模板文件(template),还有一个执行脚本文件exec.sh。 sdk文件夹：CRF++的头文件和静态链接库。 clr_learn.exe:CRF++的训练程序 crl_test.exe:CRF++的测试程序 libcrffpp.dll:训练程序和测试程序需要使用的静态链接库。 实际上，需要使用的就是crf_learn.exe，crf_test.exe和libcrfpp.dll，这三个文件。 2. 使用例子2.1 训练先拿example中的某个例子做一下测试。例如：example中 seg 文件夹，其中原有4个文件：exec.sh；template；test.data；train.data。将crf_learn.exe；crf_test.exe；libcrfpp.dll三个文件复制到这个文件夹（seg）底下。 在命令窗口中，cd到该文件夹，然后输入以下命令进行训练模型。 12crf_learn template_file train_file model_file eg:crf_learn template train.data model 你可以看到控制台上打印处的信息，并会产生一个新的文件：model。这个训练过程的时间、迭代次数等信息就会输出到控制台上（感觉是crf_learn程序的输出信息到标准输出流上了），如果想要保存这些信息，我们可以将这些标准输出流到文件，命令格式为： 模型就用它例子中的模型 12crf_learn template_file train_file model_file &gt;&gt; train_info_file eg:crf_learn template train.data model &gt;&gt; model_out.txt 在这里有四个参数可以调整：-a CRF-L2 or CRF-L1规范化算法的选择。默认是CRF-L2。一般来说L2算法效果要比L1算法稍微好一点，虽然L1算法中非零特征的数值要比L2中大幅度的小。 -c float这个参数设置CRF的hyper-parameter。c的数值越大，CRF拟合训练数据的程度越高。这个参数可以调整过拟合和不拟合之间的平衡度。这个参数可以通过交叉验证等方法寻找较优的参数。 -f NUM这个参数设置特征的cut-off threshold。CRF++使用训练数据中至少出现NUM次的特征。默认值为1。当使用CRF++到大规模数据的时候，只出现一次的特征可能会有百万个，这个选项就会在这样的情况下起作用了。 -p NUM如果电脑有多个CPU ,那么可以通过多线程提升训练速度。NUM是线程数量。 举一个带参数的命令例子：1clr_learn -f 3 -c 4.0 tempalte train.data model # 过滤掉了频数低于3的特征，并且设超参数为1.5 2.2 用测试集预测输入命令进行测试数据,测试程序的命令为： 12crf_test -m model_file test_file eg: crf_test -m model test.data 同样，与crf_learn类似，输出的结果放到了标准输出流上，而这个输出结果是最重要的预测结果信息（预测文件的内容+预测标注），同样可以使用重定向，将结果保存下来，命令为： 12crf_test -m model_file test_files &gt;&gt; result_file eg：crf_test -m model test.data &gt;&gt; output.txt 你会发现生成一个新的文件output.txt，就是我们的测试结果。 3.参考资料CRF++使用教程 Windows下CRF++ 分词实践及Python分词效果评测 中文分词入门之字标注法4 CRF++中文分词使用指南 windows10下Anaconda spyder安装CRF++的python接口]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[序列模型学习笔记]]></title>
    <url>%2F2018%2F12%2F07%2F%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[摘要：RNN, GRU, LSTM 1 RNN1.1 前向传播对于序列数据，使用标准神经网络存在以下问题： 对于不同的示例，输入和输出可能有不同的长度，因此输入层和输出层的神经元数量无法固定。 从输入文本的不同位置学到的同一特征无法共享。 模型中的参数太多，计算量太大。 为了解决这些问题，引入循环神经网络（Recurrent Neural Network，RNN）。一种循环神经网络的结构如下图所示： RNN的网络结构和神经网络一样，里面有很多神经元，较神经网络而言，每层都有两个输入，一个为上一层的输出，一个为新的输入。并且有2个输出。 一个时间序列就是一层神经网络。一个rnn处理1000个时间序列的数据集，这就是一个1000层的神经网络。 零时刻需要构造一个激活值a(0)，最常用的是零向量，也可以随机初始化。 .png) 当元素 x⟨t⟩输入对应时间步（Time Step）的隐藏层的同时，该隐藏层也会接收来自上一时间步的隐藏层的激活值 a⟨t−1⟩，其中 a⟨0⟩ 一般直接初始化为零向量。一个时间步输出一个对应的预测结果 y^⟨t⟩。 循环神经网络从左向右扫描数据，同时每个时间步的参数也是共享的，输入、激活、输出的参数对应为 Wax 、Waa、Wya。 下图是一个 RNN 神经元的结构： 前向传播过程的公式如下： 激活函数 g1通常选择 tanh，有时也用 ReLU；g2可选 sigmoid 或 softmax，取决于需要的输出类型。 为了进一步简化公式以方便运算，可以将 Waa、Wax水平并列为一个矩阵 Wa，同时 a⟨t−1⟩和 x⟨t⟩ 上下堆叠成一个矩阵。则有：(分块矩阵的写法，注意对应顺序。下图中 Wa 的写反了) 前向传播代码： 输入：xt 其中(n_x, m, T_x) n_x 为特征，m为样本数，T_x为时间步数 返回值：a_next, yt_pred, cache 中间值：因为反向传播需要，所以存储了 a_next, a_prev, xt, parameters 因为每个时间步共享参数，所以参数不变 1234567891011121314151617181920212223242526272829303132333435# 单时间步def rnn_cell_forward(xt, a_prev, parameters): """ Implements a single forward step of the RNN-cell as described in Figure (2) Arguments: xt -- your input data at timestep "t", numpy array of shape (n_x, m). a_prev -- Hidden state at timestep "t-1", numpy array of shape (n_a, m) parameters -- python dictionary containing: Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x) Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a) Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a) ba -- Bias, numpy array of shape (n_a, 1) by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1) Returns: a_next -- next hidden state, of shape (n_a, m) yt_pred -- prediction at timestep "t", numpy array of shape (n_y, m) cache -- tuple of values needed for the backward pass, contains (a_next, a_prev, xt, parameters) """ # Retrieve parameters from "parameters" Wax = parameters["Wax"] Waa = parameters["Waa"] Wya = parameters["Wya"] ba = parameters["ba"] by = parameters["by"] a_next = np.tanh(np.dot(Wax, xt) + np.dot(Waa, a_prev) + ba) # compute output of the current cell using the formula given above yt_pred = softmax(np.dot(Wya, a_next) + by) # store values you need for backward propagation in cache cache = (a_next, a_prev, xt, parameters) return a_next, yt_pred, cache 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# 多时间步def rnn_forward(x, a0, parameters): """ Implement the forward propagation of the recurrent neural network described in Figure (3). Arguments: x -- Input data for every time-step, of shape (n_x, m, T_x). a0 -- Initial hidden state, of shape (n_a, m) parameters -- python dictionary containing: Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a) Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x) Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a) ba -- Bias numpy array of shape (n_a, 1) by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1) Returns: a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x) y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x) caches -- tuple of values needed for the backward pass, contains (list of caches, x) """ # Initialize "caches" which will contain the list of all caches caches = [] # Retrieve dimensions from shapes of x and parameters["Wya"] n_x, m, T_x = x.shape n_y, n_a = parameters["Wya"].shape ### START CODE HERE ### # initialize "a" and "y" with zeros (≈2 lines) a = np.zeros((n_a, m, T_x)) y_pred = np.zeros((n_y, m, T_x)) # Initialize a_next (≈1 line) a_next = a0 # loop over all time-steps for t in range(T_x): # Update next hidden state, compute the prediction, get the cache (≈1 line) a_next, yt_pred, cache = rnn_cell_forward(x[:,:,t], a_next, parameters) # Save the value of the new "next" hidden state in a (≈1 line) a[:,:,t] = a_next # Save the value of the prediction in y (≈1 line) y_pred[:,:,t] = yt_pred # Append "cache" to "caches" (≈1 line) caches.append(cache) # store values needed for backward propagation in cache caches = (caches, x) return a, y_pred, caches 1.2 反向传播：为了计算反向传播过程，需要先定义一个损失函数。单个位置上（或者说单个时间步上）某个单词的预测值的损失函数采用交叉熵损失函数，如下所示： 循环神经网络的反向传播被称为通过时间反向传播（Backpropagation through time），因为从右向左计算的过程就像是时间倒流。 更详细的计算公式如下： 1.3 梯度消失 对于以上两个句子，后面的动词单复数形式由前面的名词的单复数形式决定。但是基本的 RNN 不擅长捕获这种长期依赖关系。究其原因，由于梯度消失，在反向传播时，很深的神经网络，从输出y^得到的梯度很难传播回去，很难影响靠前层的权重，后面层的输出误差很难影响到较靠前层的计算，网络很难调整靠前的计算。所以很难让它记住是单数还是复数。 在反向传播时，随着层数的增多，梯度不仅可能指数型下降，也有可能指数型上升，即梯度爆炸。不过梯度爆炸比较容易发现，因为参数会急剧膨胀到数值溢出（可能显示为 NaN）。这时可以采用梯度修剪（Gradient Clipping）来解决：观察梯度向量，如果它大于某个阈值，则缩放梯度向量以保证其不会太大。相比之下，梯度消失问题更难解决。GRU 和 LSTM 都可以作为缓解梯度消失问题的方案。 2 GRUGRU（Gated Recurrent Units, 门控循环单元）改善了 RNN 的隐藏层，使其可以更好地捕捉深层连接，并改善了梯度消失问题。 当我们从左到右读上面这个句子时，GRU 单元有一个新的变量称为 c，代表记忆细胞（Memory Cell），其作用是提供记忆的能力，记住例如前文主语是单数还是复数等信息。在时间 t，记忆细胞的值 c⟨t⟩等于输出的激活值 a⟨t⟩；c~⟨t⟩ 代表下一个 c 的候选值。Γu 代表更新门（Update Gate），用于决定什么时候更新记忆细胞的值。以上结构的具体公式为： 当使用 sigmoid 作为激活函数 σ 来得到 Γu时，Γu的值在 0 到 1 的范围内，且大多数时间非常接近于 0 或 1。当 Γu=1时，c⟨t⟩被更新为 c~⟨t⟩，否则保持为 c⟨t−1⟩。因为 Γu可以很接近 0，因此 c⟨t⟩几乎就等于 c⟨t−1⟩。在经过很长的序列后，c的值依然被维持，从而实现“记忆”的功能。 因为sigmoid的值很容易取到0，或者非常接近0，这时c(t)=c(t-1) ,有利于维持细胞的值。Γu很接近0，但不是0 ，就不会有梯度消失的问题了。有效缓解了梯度消失的问题。 以上实际上是简化过的 GRU 单元，但是蕴涵了 GRU 最重要的思想。完整的 GRU 单元添加了一个新的相关门（Relevance Gate） Γr，表示 c~⟨t⟩和 c⟨t-1⟩的相关性。因此，表达式改为如下所示： 3 LSTMLSTM（Long Short Term Memory，长短期记忆）网络比 GRU 更加灵活和强大，它额外引入了遗忘门（Forget Gate） ΓfΓf和输出门（Output Gate） ΓoΓo。其结构图和公式如下：]]></content>
      <categories>
        <category>DeepLearning</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>DeepLearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[知识图谱构建技术综述]]></title>
    <url>%2F2018%2F12%2F05%2F%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%9E%84%E5%BB%BA%E6%8A%80%E6%9C%AF%E7%BB%BC%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[摘要：知识图谱构建的基本介绍 1 知识图谱的定义与架构1.1 定义知识图谱．是结构化的语义知识库用于以符号形式描述物理世界中的概念、实体、事件及其相互关系,本质上，知识图谱是一种揭示实体之间关系的语义网络，可以对现实世界的事物及其相互关系进行形式化地描述。现在的知识图谱已被用来泛指各种大规模的知识库。其基本组成单位是实体－关系－实体三元组以及实体及其相关属性－值对实体间通过关系相互联结构成网状的知识结构。其中概念是指人们在认识世界过程中形成的对客观事物的概念化表示，如人、动物、组织机构等；实体是客观世界中的具体事物，如篮球运动员姚明、互联网公司腾讯等；事件是客观世界的活动，如地震、买卖行为等；关系描述概念、实体、事件之间客观存在的关联，如毕业院校描述了个人与其所在院校的关系，运动员和篮球运动员之间概念和子概念的关系等。 通过知识图谱可以实现Ｗeb从网页链接向概念链接转变，支持用户按主题而不是字符串检索，从而真正实现语义检索．基于知识图谱的搜索引擎能够以图形方式向用户反馈结构化的知识，用户不必浏览大量网页就可以准确定位和深度获取知识。 包括三层含义： 知识图谱本身是一个具有属性的实体通过关系链接而成的网状知识库．从图的角度来看知识图谱在本质上是一种概念网络，其中的节点表示物理世界的实体或概念，而实体间的各种语义关系则构成网络中的边．由此知识图谱是对物理世界的一种符号表达。 知识图谱的研究价值在于它是构建在当前Ｗeｂ基础之上的一层覆盖网络，借助知识图谱能够在Ｗeｂ网页之上建立概念间的链接关系，从而以最小的代价将互联网中积累的信息组织起来，成为可以被利用的知识。 知识图谱的应用价值在于它能够改变现有的信息检索方式。一方面通过推理实现概念检索（相对于现有的字符串模糊匹配方式而言）， 另一方面以图形化方式向用户展示经过分类整理的结构化知识，从而使人们从人工过滤网页寻找答案的模式中解脱出来。 三元组是知识图谱的一种通用表示方式，即 G = (E,R,S)，其中 E={e1,e2, … ,e|E|} 是知识库中的实体集合，共包含|E|种不同实体；R ={r1,r2 … ,r|E|}是知识库中的关系集合，共包|R|种不同关系; S代表知识库中的三元组集合。三元组的基本形式主要包括实体1、关系、实体2和概念、属性、属性值等，实体是知识图谱中的最基本元素，不同的实体间存在不同的关系。概念主要指集合、类别、对象类型、事物的种类，例如人物、地理等；属性主要指对象可能具有的属性、特征、特性、特点以及参数，例如国籍、生日等；属性值主要指对象指定属性的值，例如中国等。每个实体(概念的外延)可用一个全局唯一确定的ID来标识，每个属性-属性值对(attribute-value pair，AV P )可用来刻画实体的内在特性，而关系可用来连接两个实体，刻画它们之间的关联。 1.2 架构知识图谱的架构包括知识图谱自身的逻辑结构以及构建知识图谱所采用的技术（体系）架构，后者是本文讨论的重点。 1.2.1 逻辑架构首先介绍知识图谱的逻辑结构从逻辑上将知识图谱划分为２个层次数据层和模式层。 1.2.1.1 数据层在知识图谱数据层，知识以事实(fact)为单位存储在图数据库．如果以“实体－关系－实体“或者“实体－属性－性值“三元组作为事实的基本表达方式，则存储在图数据库中的所有数据将构成庞大的实体关系网络形成知识的图谱。 1.2.1.2 模式层模式层在数据层之上是知识图谱的核心．在模式层存储的是经过提炼的知识通常采用本体库（有一匹马叫赤兔，那么马这个概念才是本体；有一个美女叫貂蝉，那么美女这个概念才是本体；本体就是知识库本身的存在，和里面的数据没有关系）来管理知识图谱的模式层，借助本体库对公理、规则和约束条件的支持能力来规范实体、关系以及实体的类型和属性等对象之间的联系。本体库在知识图谱中的地位相当于知识库的模具拥有本体库的知识库（知识数据库，包含了知识的本体和知识。Freebase是一个知识库（结构化），维基百科也可以看成一个知识库（半结构化））冗余知识较少。 1.2.2 技术架构 知识图谱有自顶向下和自底向上２种构建方式．所谓自顶向下构建是指借助百科类网站等结构化数据源，从高质量数据中提取本体和模式信息，加入到知识库中；所谓自底向上构建则是借助一定的技术手段从公开采集的数据中提取出资源模式，选择其中置信度较高的新模式经人工审核之后加入到知识库中。 2 知识图谱的构建技术采用自底向上的方式构建知识图谱的过程是一个迭代更新的过程每一轮更新包括３个步骤： 信息抽取 ，即从各种类型的数据源中提取出实体(概念)、 属性以及实体间的相互关系 ，在此基础上形成本体化的知识表达 知识融合 ，在获得新知识之后需要对其进行整合以消除矛盾和歧义 ，比如某些实体可能有多种表达，某个特定称谓也许对应于多个不同的实体等 知识加工 ，对于经过融合的新知识，需要经过质量评估之后（部分需要人工参与甄别）才能将合格的部分加入到知识库中以确保知识库的质量．新增数据之后，可以进行知识推理拓展现有知识得到新知识。 2.1 信息抽取信息抽取是知识图谱构建的第１步，其中的关键问题是如何从异构数据源中自动抽取信息得到候选知识单元．信息抽取是一种自动化地从半结构化和无结构数据中抽取实体、关系以及实体属性等结构化信息的技术。涉及的关键技术包括实体抽取、关系抽取和属性抽取。 2.1.1 实体抽取实体抽取，也称为命名实体识别(named entity recognition, NER) ，是指从文本数据集中自动识别出命名实体．实体抽取的质量（准确率和召回率）对后续的知识获取效率和质量影响极大，因此是信息抽取中最为基础和关键的部分。 2.1.2 关系抽取文本语料经过实体抽取得到的是一系列离散的命名实体，为了得到语义信息还需要从相关语料中提取出实体之间的关联关系，通过关系将实体（概念）联系起来，才能够形成网状的知识结构。研究关系抽取技术的目的就是解决如何从文本语料中抽取实体间的关系这一基本问题。 2.1.3 属性抽取属性抽取的目标是从不同信息源中采集特定实体的属性信息．例如针对某个公众人物，可以从网络公开信息中得到其昵称、生日、国籍、教育背景等信息。属性抽取技术能够从多种数据来源中汇集这些信息，实现对实体属性的完整勾画。 由于可以将实体的属性视为实体与属性值之间的一种名词性关系，因此也可以将属性抽取问题视为关系抽取问题。 2.2 知识融合通过信息抽取，实现了从非结构化和半结构化数据中获取实体、关系以及实体属性信息的目标，然而这些结果中可能包含大量的冗余和错误信息，数据之间的关系也是扁平化的缺乏层次性和逻辑性，因此有必要对其进行清理和整合。知识融合包括２部分内容：实体链接和知识合并。通过知识融合，可以消除概念的歧义，剔除冗余和错误概念，从而确保知识的质量 。 2.2.1 实体链接实体链接（entity linking）是指对于从文本中抽取得到的实体对象，将其链接到知识库中对应的正确实体对象的操作。 实体链接的基本思想是首先根据给定的实体指称项，从知识库中选出一组候选实体对象然后通过相似度计算将指称项链接到正确的实体对象。 实体链接的一般流程是： 从文本中通过实体抽取得到实体指称项 进行实体消歧和共指消解 ，判断知识库中的同名实体与之是否代表不同的含义，以及知识库中是否存在其他命名实体与之表示相同的含义 在确认知识库中对应的正确实体对象之后，将该实体指称项链接到知识库中对应实体 。 2.2.1.1 实体消歧实体消歧（entity disambiguation）是专门用于解决同名实体产生歧义问题的技术．在实际语言环境中经常会遇到某个实体指称项对应于多个命名实体对象的问题。例如李娜这个名词（指称项）可以对应于作为歌手的李娜这个实体，也可以对应于作为网球运动员的李娜这个实体，通过实体消歧就可以根据当前的语境准确建立实体链接．实体消歧主要采用聚类法 。 聚类法是指以实体对象为聚类中心将所有指向同一目标实体对象的指称项聚集到以该对象为中心的类别下。聚类法消歧的关键问题是如何定义实体对象与指称项之间的相似度 ，常用方法有４种。 空间向量模型词袋模型。典型的方法是取当前语料中实体指称项周边的词构成特征向量，然后利用向量的余弦相似度进行比较，将该指称项聚类到与之最相近的实体指称项集合中。 语义模型 。该模型与空间向量模型类似，区别在于特征向量的构造方法不同，语义模型的特征向量不仅包含词袋向量而且包含一部分语义特征。 社会网络模型 。该模型的基本假设是物以类聚、人以群分，在社会化语境中，实体指称项的意义在很大程度上是由与其相关联的实体所决定的．建模时，首先利用实体间的关系将与之相关的指称项链接起来构成网络然，后利用社会网络分析技术计算该网络中节点之间的拓扑距离（网络中的节点即实体的指称项），以此来判定指称项之间的相似度。 百科知识模型．百科类网站通常会为每个实体（指称项）分配一个单独页面，其中包括指向其他实体页面的超链接，百科知识模型正是利用这种链接关系来计算实体指称项之间的相似度。 2.2.1.2 共指消解共指消解（entity resolution） 技术主要用于解决多个指称项对应于同一实体对象的问题。例如在一篇新闻稿中“Barack Obama”, “president Obama”, “the president”等指称项可能指向的是同一实体对象，其中的许多代词如”he”，”him” 等也可能指向该实体对象．利用共指消解技术可以将这些指称项关联(合并)到正确的实体对象．由于该问题在信息检索和自然语言处理等领域具有特殊的重要性，吸引了大量的研究努力，因此学术界对该问题有多种不同的表述典型的包括：对象对齐(object alignment)、 实体匹配(entity matching)、以及实体同义（entity synonyms）。 2.2.2 知识合并在构建知识图谱时，可以从第三方知识库产品或已有结构化数据获取知识输入。 2.2.2.1 合并外部知识库将外部知识库融合到本地知识库需要处理２个层面的问题． 数据层的融合，包括实体的指称、属性、关系以及所属类别等，主要的问题是如何避免实例以及关系的冲突问题，造成不必要的冗余 通过模式层的融合将新得到的本体融入已有的本体库中。 2.2.2.2 合并关系数据库在知识图谱构建过程中一个重要的高质量知识来源是企业或者机构自己的关系数据库。 2.3 知识加工通过信息抽取，可以从原始语料中提取出实体、关系与属性等知识要素．再经过知识融合，可以消除实体指称项与实体对象之间的歧义得到一系列基本的事实表达．然而，事实本身并不等于知识，要想最终获得结构化、网络化的知识体系，还需要经历知识加工的过程．知识加工主要包括３方面内容：本体构建、知识推理和质量评估。 2.3.1 本体构建本体（ontology）是对概念进行建模的规范，是描述客观世界的抽象模型，以形式化方式对概念及其之间的联系给出明确定义．本体的最大特点在于它是共享的，本体中反映的知识是一种明确定义的共识．虽然在不同时代和领域学者们对本体曾经给出过不同的定义，但这些定义的内涵是一致的，即：本体是同一领域内的不同主体之间进行交流的语义基础．本体是树状结构，相邻层次的节点（概念）之间具有严格的“IsA” 关系，这种单纯的关系有助于知识推理，但却不利于表达概念的多样性．在知识图谱中，本体位于模式层，用于描述概念层次体系是知识库中知识的概念模板。 2.3.2 知识推理知识推理是指从知识库中已有的实体关系数据出发，经过计算机推理建立实体间的新关联，从而拓展和丰富知识网络．知识推理是知识图谱构建的重要手段和关键环节，通过知识推理，能够从现有知识中发现新的知识．例如已知（乾隆，父亲，雍正）和（雍正，父亲，康熙）可以得到（乾隆，祖父，康熙）或（康熙，孙子，乾隆）．知识推理的对象并不局限于实体间的关系也可以是实体的属性值、本体的概念层次关系等．例如已知某实体的生日属性，可以通过推理得到该实体的年龄属性．根据本体库中的概念继承关系，也可以进行概念推理，例如已知（老虎，科，猫科）和（猫科，目，食肉目）可以推出（老虎，目，食肉目）。 知识的推理方法可以分为２大类：基于逻辑的推理和基于图的推理 2.3.3 质量评估质量评估也是知识库构建技术的重要组成部分． 受现有技术水平的限制，采用开放域信息抽取技术得到的知识元素有可能存在错误（如实体识别错误、关系抽取错误等）经过知识推理得到的知识的质量同样也是没有保障的，因此在将其加入知识库之前需要有一个质量评估的过程 随着开放关联数据项目的推进，各子项目所产生的知识库产品间的质量差异也在增大，数据间的冲突日益增多，如何对其质量进行评估，对于全局知识图谱的构建起着重要的作用． 引入质量评估的意义在于：可以对知识的可信度进行量化 ，通过舍弃置信度较低的知识，可以保障知识库的质量。 2.4 知识更新人类所拥有的信息和知识量都是时间的单调递增函数，因此知识图谱的内容也需要与时俱进，其构建过程是一个不断迭代更新的过程。 从逻辑上看知识库的更新包括概念层的更新和数据层的更新．概念层的更新是指新增数据后获得了新的概念，需要自动将新的概念添加到知识库的概念层中．数据层的更新主要是新增或更新实体、关系和属性值，对数据层进行更新需要考虑数据源的可靠性，数据的一致性（是否存在矛盾或冗余等问题）等多方面因素．当前流行的方法是选择百科类网站等可靠数据源，并选择在各数据源中出现频率高的事实和属性加入知识库．知识的更新也可以采用众包的模式（如 Freebase），而对于概念层的更新，则需要借助专业团队进行人工审核。 知识图谱的内容更新有２种方式数据驱动下的全面更新和增量更新．所谓全面更新是指以更新后的全部数据为输入，从零开始构建知识图谱．这种方式比较简单，但资源消耗大，而且需要耗费大量人力资源进行系统维护；而增量更新，则是以当前新增数据为输入，向现有知识图谱中添加新增知识．这种方式资源消耗小，但目前仍需要大量人工干预（定义规则等，因此实施起来十分困难。 3 参考资料知识图谱构建技术综述 刘峤 李杨 段宏 刘瑶 秦志光]]></content>
      <categories>
        <category>知识图谱</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>知识图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聊天机器人]]></title>
    <url>%2F2018%2F11%2F26%2F%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA%2F</url>
    <content type="text"><![CDATA[摘要：制作个人聊天机器人笔记 1 聊天机器人简介1.1 工作原理1.1.1 核心模块 意图识别 分类器（SVM，深度学习） 实体识别 NER（命名实体识别） 对话管理系统 回复生成 1.2 关键技术 海量文本知识表示：网络文本资源获取、机器学习方法、大规模语义计算和推理、知识表示体系、知识库构建； 问句解析：中文分词、词性标注、实体标注、概念类别标注、句法分析、语义分析、逻辑结构标注、指代消解、关联关系标注、问句分类（简单问句还是复杂问句、实体型还是段落型还是篇章级问题）、答案类别确定； 答案生成与过滤：候选答案抽取、关系推演（并列关系还是递进关系还是因果关系）、吻合程度判断、噪声过滤 1.3 技术方法 基于检索的技术（淘汰） 基于模式匹配的技术（淘汰） 基于意图识别的⽅法 ⽣成式⽅法（i.e.,端到端） 2 词性标注和关键词提取2.1 安装PyNLPIR官网地址 1pip install PyNLPIR 2.2 初始化NLPIR123import pynlpir# 默认情况下，输入假定为unicode或UTF-8编码。如果您想使用不同的编码（例如GBK或BIG5）pynlpir.open(encoding=&apos;gbk&apos;) 2.3 分词及词性标注1234# 词性标注 pos_tagging=True；词性标注显示英文/中文 pos_english=True； 词性标记的显示方式 pos_names='parent/child/all'# 返回的是tuple(token, pos)组成的列表，其中token就是切出来的词，pos就是语言属性# 调用segment方法指定的pos_names参数可以是'all', 'child', 'parent'，默认是parent， 表示获取该词性的最顶级词性，child表示获取该词性的最具体的信息，all表示获取该词性相关的所有词性信息，相当于从其顶级词性到该词性的一条路径pynlpir.segment(s, pos_tagging=True, pos_names='parent', pos_english=True) 1234s = &apos;NLPIR分词系统前身为2000年发布的ICTCLAS词法分析系统，从2009年开始，为了和以前工作进行大的区隔，并推广NLPIR自然语言处理与信息检索共享平台，调整命名为NLPIR分词系统。&apos;pynlpir.segment(s)# Sample output: [(&apos;NLPIR&apos;, &apos;noun&apos;), (&apos;分词&apos;, &apos;verb&apos;), (&apos;系统&apos;, &apos;noun&apos;), (&apos;前身&apos;, &apos;noun&apos;), (&apos;为&apos;, &apos;preposition&apos;), (&apos;2000年&apos;, &apos;time word&apos;), (&apos;发布&apos;, &apos;verb&apos;), . . . ] 如果不想词性标注，设置post_tagging为false： 123pynlpir.segment(s, pos_tagging=False)# Sample output: ['NLPIR', '分词', '系统', '前身', '为', '2000年', '发布', . . . ] 2.4 关键字提取1234# 获得多少个词：max_words=50； 显示关键字权重：weighted=Truepynlpir.get_key_words(s, max_words=50, weighted=True)# 关闭APIpynlpir.close() 2.5 词性分类表123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116POS_MAP = &#123; &apos;n&apos;: (&apos;名词&apos;, &apos;noun&apos;, &#123; &apos;nr&apos;: (&apos;人名&apos;, &apos;personal name&apos;, &#123; &apos;nr1&apos;: (&apos;汉语姓氏&apos;, &apos;Chinese surname&apos;), &apos;nr2&apos;: (&apos;汉语名字&apos;, &apos;Chinese given name&apos;), &apos;nrj&apos;: (&apos;日语人名&apos;, &apos;Japanese personal name&apos;), &apos;nrf&apos;: (&apos;音译人名&apos;, &apos;transcribed personal name&apos;) &#125;), &apos;ns&apos;: (&apos;地名&apos;, &apos;toponym&apos;, &#123; &apos;nsf&apos;: (&apos;音译地名&apos;, &apos;transcribed toponym&apos;), &#125;), &apos;nt&apos;: (&apos;机构团体名&apos;, &apos;organization/group name&apos;), &apos;nz&apos;: (&apos;其它专名&apos;, &apos;other proper noun&apos;), &apos;nl&apos;: (&apos;名词性惯用语&apos;, &apos;noun phrase&apos;), &apos;ng&apos;: (&apos;名词性语素&apos;, &apos;noun morpheme&apos;), &#125;), &apos;t&apos;: (&apos;时间词&apos;, &apos;time word&apos;, &#123; &apos;tg&apos;: (&apos;时间词性语素&apos;, &apos;time morpheme&apos;), &#125;), &apos;s&apos;: (&apos;处所词&apos;, &apos;locative word&apos;), &apos;f&apos;: (&apos;方位词&apos;, &apos;noun of locality&apos;), &apos;v&apos;: (&apos;动词&apos;, &apos;verb&apos;, &#123; &apos;vd&apos;: (&apos;副动词&apos;, &apos;auxiliary verb&apos;), &apos;vn&apos;: (&apos;名动词&apos;, &apos;noun-verb&apos;), &apos;vshi&apos;: (&apos;动词&quot;是&quot;&apos;, &apos;verb 是&apos;), &apos;vyou&apos;: (&apos;动词&quot;有&quot;&apos;, &apos;verb 有&apos;), &apos;vf&apos;: (&apos;趋向动词&apos;, &apos;directional verb&apos;), &apos;vx&apos;: (&apos;行事动词&apos;, &apos;performative verb&apos;), &apos;vi&apos;: (&apos;不及物动词&apos;, &apos;intransitive verb&apos;), &apos;vl&apos;: (&apos;动词性惯用语&apos;, &apos;verb phrase&apos;), &apos;vg&apos;: (&apos;动词性语素&apos;, &apos;verb morpheme&apos;), &#125;), &apos;a&apos;: (&apos;形容词&apos;, &apos;adjective&apos;, &#123; &apos;ad&apos;: (&apos;副形词&apos;, &apos;auxiliary adjective&apos;), &apos;an&apos;: (&apos;名形词&apos;, &apos;noun-adjective&apos;), &apos;ag&apos;: (&apos;形容词性语素&apos;, &apos;adjective morpheme&apos;), &apos;al&apos;: (&apos;形容词性惯用语&apos;, &apos;adjective phrase&apos;), &#125;), &apos;b&apos;: (&apos;区别词&apos;, &apos;distinguishing word&apos;, &#123; &apos;bl&apos;: (&apos;区别词性惯用语&apos;, &apos;distinguishing phrase&apos;), &#125;), &apos;z&apos;: (&apos;状态词&apos;, &apos;status word&apos;), &apos;r&apos;: (&apos;代词&apos;, &apos;pronoun&apos;, &#123; &apos;rr&apos;: (&apos;人称代词&apos;, &apos;personal pronoun&apos;), &apos;rz&apos;: (&apos;指示代词&apos;, &apos;demonstrative pronoun&apos;, &#123; &apos;rzt&apos;: (&apos;时间指示代词&apos;, &apos;temporal demonstrative pronoun&apos;), &apos;rzs&apos;: (&apos;处所指示代词&apos;, &apos;locative demonstrative pronoun&apos;), &apos;rzv&apos;: (&apos;谓词性指示代词&apos;, &apos;predicate demonstrative pronoun&apos;), &#125;), &apos;ry&apos;: (&apos;疑问代词&apos;, &apos;interrogative pronoun&apos;, &#123; &apos;ryt&apos;: (&apos;时间疑问代词&apos;, &apos;temporal interrogative pronoun&apos;), &apos;rys&apos;: (&apos;处所疑问代词&apos;, &apos;locative interrogative pronoun&apos;), &apos;ryv&apos;: (&apos;谓词性疑问代词&apos;, &apos;predicate interrogative pronoun&apos;), &#125;), &apos;rg&apos;: (&apos;代词性语素&apos;, &apos;pronoun morpheme&apos;), &#125;), &apos;m&apos;: (&apos;数词&apos;, &apos;numeral&apos;, &#123; &apos;mq&apos;: (&apos;数量词&apos;, &apos;numeral-plus-classifier compound&apos;), &#125;), &apos;q&apos;: (&apos;量词&apos;, &apos;classifier&apos;, &#123; &apos;qv&apos;: (&apos;动量词&apos;, &apos;verbal classifier&apos;), &apos;qt&apos;: (&apos;时量词&apos;, &apos;temporal classifier&apos;), &#125;), &apos;d&apos;: (&apos;副词&apos;, &apos;adverb&apos;), &apos;p&apos;: (&apos;介词&apos;, &apos;preposition&apos;, &#123; &apos;pba&apos;: (&apos;介词“把”&apos;, &apos;preposition 把&apos;), &apos;pbei&apos;: (&apos;介词“被”&apos;, &apos;preposition 被&apos;), &#125;), &apos;c&apos;: (&apos;连词&apos;, &apos;conjunction&apos;, &#123; &apos;cc&apos;: (&apos;并列连词&apos;, &apos;coordinating conjunction&apos;), &#125;), &apos;u&apos;: (&apos;助词&apos;, &apos;particle&apos;, &#123; &apos;uzhe&apos;: (&apos;着&apos;, &apos;particle 着&apos;), &apos;ule&apos;: (&apos;了／喽&apos;, &apos;particle 了/喽&apos;), &apos;uguo&apos;: (&apos;过&apos;, &apos;particle 过&apos;), &apos;ude1&apos;: (&apos;的／底&apos;, &apos;particle 的/底&apos;), &apos;ude2&apos;: (&apos;地&apos;, &apos;particle 地&apos;), &apos;ude3&apos;: (&apos;得&apos;, &apos;particle 得&apos;), &apos;usuo&apos;: (&apos;所&apos;, &apos;particle 所&apos;), &apos;udeng&apos;: (&apos;等／等等／云云&apos;, &apos;particle 等/等等/云云&apos;), &apos;uyy&apos;: (&apos;一样／一般／似的／般&apos;, &apos;particle 一样/一般/似的/般&apos;), &apos;udh&apos;: (&apos;的话&apos;, &apos;particle 的话&apos;), &apos;uls&apos;: (&apos;来讲／来说／而言／说来&apos;, &apos;particle 来讲/来说/而言/说来&apos;), &apos;uzhi&apos;: (&apos;之&apos;, &apos;particle 之&apos;), &apos;ulian&apos;: (&apos;连&apos;, &apos;particle 连&apos;), &#125;), &apos;e&apos;: (&apos;叹词&apos;, &apos;interjection&apos;), &apos;y&apos;: (&apos;语气词&apos;, &apos;modal particle&apos;), &apos;o&apos;: (&apos;拟声词&apos;, &apos;onomatopoeia&apos;), &apos;h&apos;: (&apos;前缀&apos;, &apos;prefix&apos;), &apos;k&apos;: (&apos;后缀&apos; &apos;suffix&apos;), &apos;x&apos;: (&apos;字符串&apos;, &apos;string&apos;, &#123; &apos;xe&apos;: (&apos;Email字符串&apos;, &apos;email address&apos;), &apos;xs&apos;: (&apos;微博会话分隔符&apos;, &apos;hashtag&apos;), &apos;xm&apos;: (&apos;表情符合&apos;, &apos;emoticon&apos;), &apos;xu&apos;: (&apos;网址URL&apos;, &apos;URL&apos;), &apos;xx&apos;: (&apos;非语素字&apos;, &apos;non-morpheme character&apos;), &#125;), &apos;w&apos;: (&apos;标点符号&apos;, &apos;punctuation mark&apos;, &#123; &apos;wkz&apos;: (&apos;左括号&apos;, &apos;left parenthesis/bracket&apos;), &apos;wky&apos;: (&apos;右括号&apos;, &apos;right parenthesis/bracket&apos;), &apos;wyz&apos;: (&apos;左引号&apos;, &apos;left quotation mark&apos;), &apos;wyy&apos;: (&apos;右引号&apos;, &apos;right quotation mark&apos;), &apos;wj&apos;: (&apos;句号&apos;, &apos;period&apos;), &apos;ww&apos;: (&apos;问号&apos;, &apos;question mark&apos;), &apos;wt&apos;: (&apos;叹号&apos;, &apos;exclamation mark&apos;), &apos;wd&apos;: (&apos;逗号&apos;, &apos;comma&apos;), &apos;wf&apos;: (&apos;分号&apos;, &apos;semicolon&apos;), &apos;wn&apos;: (&apos;顿号&apos;, &apos;enumeration comma&apos;), &apos;wm&apos;: (&apos;冒号&apos;, &apos;colon&apos;), &apos;ws&apos;: (&apos;省略号&apos;, &apos;ellipsis&apos;), &apos;wp&apos;: (&apos;破折号&apos;, &apos;dash&apos;), &apos;wb&apos;: (&apos;百分号千分号&apos;, &apos;percent/per mille sign&apos;), &apos;wh&apos;: (&apos;单位符号&apos;, &apos;unit of measure sign&apos;), &#125;), &#125; 3 通过爬虫获取语料信息通过上节得到了关键词，想获取预料信息，通过几大搜索引擎的调用接口获取。 3.1 Anaconda安装Scrapy 首先查看anaconda中是否装有scrapy工具包，具体方法如下：Anaconda Prompt / cmd命令中，输入 conda list,查看所有已经安装的工具包及版本号 如果没有发现scrapy，则执行第2步。 输入 conda install -c scrapinghub scrapy ，等待片刻后，提示需要安装的相关工具包 proceed下输入y，回车， 自动进行安装相关的库。 再一次通过conda list 查看，就可以看到scrapy已经在list中了。安装成功。 3.2 创建Scrapy项目 打开Anaconda Prompt，切换到想要创建的目录下面 12345cd D:\WorkSpace\Spider\# 执行下面语句创建scrapy工程scrapy startproject baidu_search# 将会自动生成了baidu_search目录和下面的文件创建baidu_search/baidu_search/spiders/baidu_search.py文件并对其进行配置，做好抓取器。 参考资料 进入baidu_search/baidu_search/ 目录下，执行 1scrapy crawl baidu_search 4 依存句法和语义依存分析4.1 依存句法分析依存句法就是这些成分之间有一种依赖关系。什么是依赖：没有你的话，我存在就是个错误。“北京是中国的首都”，如果没有“首都”，那么“中国的”存在就是个错误，因为“北京是中国的”表达的完全是另外一个意思了。 4.2 语义依存分析“语义”就是说句子的含义，“张三昨天告诉李四一个秘密”，那么语义包括：谁告诉李四秘密的？张三。张三告诉谁一个秘密？李四。张三什么时候告诉的？昨天。张三告诉李四什么？秘密。 4.3 语义依存和依存句法的区别依存句法强调介词、助词等的划分作用，语义依存注重实词之间的逻辑关系。 另外，依存句法随着字面词语变化而不同，语义依存不同字面词语可以表达同一个意思，句法结构不同的句子语义关系可能相同。 4.4 依存句法分析和语义依存分析对聊天机器人有什么意义呢？依存句法分析和语义分析相结合使用，对对方说的话进行依存句法和语义分析后，一方面可以让计算机理解句子的含义，从而匹配到最合适的回答，另外如果有已经存在的依存句法、语义分析结果，还可以通过置信度匹配来实现聊天回答。 4.5 依存句法分析到底是怎么分析的呢？依存句法分析的基本任务是确定句式的句法结构(短语结构)或句子中词汇之间的依存关系。依存句法分析最重要的两棵树： 依存树：子节点依存于父节点 依存投射树：实线表示依存联结关系，位置低的成分依存于位置高的成分，虚线为投射线 4.6 依存关系的五条公理 一个句子中只有一个成分是独立的 其他成分直接依存于某一成分 任何一个成分都不能依存于两个或两个以上的成分 如果A成分直接依存于B成分，而C成分在句子中位于A和B之间，那么C或者直接依存于B，或者直接依存于A和B之间的某一成分 中心成分左右两面的其他成分相互不发生关系 什么地方存在依存关系呢？比如合成词（如：国内）、短语（如：英雄联盟）很多地方都是 4.7 LTP依存关系标记 4.8 那么依存关系是怎么计算出来的呢？是通过机器学习和人工标注来完成的，机器学习依赖人工标注，那么都哪些需要我们做人工标注呢？分词词性、依存树库、语义角色都需要做人工标注，有了这写人工标注之后，就可以做机器学习来分析新的句子的依存句法了 4.9 LTP云平台怎么用？首先注册用户，得到每月免费20G的流量，在http://www.ltp-cloud.com/注册一个账号，注册好后登陆并进入你的dashboard：http://www.ltp-cloud.com/dashboard/在dashboard里还可以查询自己流量使用情况。具体使用方法如下（参考http://www.ltp-cloud.com/document）： 5 语言模型业界目前比较认可而且有效的语言模型是n元语法模型(n-gram model)，它本质上是马尔可夫模型，简单来描述就是：一句话中下一个词的出现和最近n个词有关(包括它自身)。详细解释一下： 如果这里的n=1时，那么最新一个词只和它自己有关，也就是它是独立的，和前面的词没关系，这叫做一元文法 如果这里的n=2时，那么最新一个词和它前面一个词有关，比如前面的词是“我”，那么最新的这个词是“是”的概率比较高，这叫做二元文法，也叫作一阶马尔科夫链 依次类推，工程上n=3用的是最多的，因为n越大约束信息越多，n越小可靠性更高 n元语法模型实际上是一个概率模型，也就是出现一个词的概率是多少，或者一个句子长这个样子的概率是多少。 这就又回到了之前文章里提到的自然语言处理研究的两大方向：基于规则、基于统计。n元语法模型显然是基于统计的方向。 5.1 语言模型的应用这几乎就是自然语言处理的应用了，有：中文分词、机器翻译、拼写纠错、语音识别、音子转换、自动文摘、问答系统、OCR等]]></content>
      <categories>
        <category>聊天机器人</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy学习笔记]]></title>
    <url>%2F2018%2F11%2F26%2FScrapy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[摘要：Scrapy 的安装及创建工程 1 Anaconda安装Scrapy 首先查看anaconda中是否装有scrapy工具包，具体方法如下：Anaconda Prompt / cmd命令中，输入 conda list,查看所有已经安装的工具包及版本号 如果没有发现scrapy，则执行第2步。 输入 conda install -c scrapinghub scrapy ，等待片刻后，提示需要安装的相关工具包 proceed下输入y，回车， 自动进行安装相关的库。 再一次通过conda list 查看，就可以看到scrapy已经在list中了。安装成功。 2 创建Scrapy项目 打开Anaconda Prompt，切换到想要创建的目录下面 12345cd D:\WorkSpace\Spider\# 执行下面语句创建scrapy工程scrapy startproject baidu_search# 将会自动生成了baidu_search目录和下面的文件创建baidu_search/baidu_search/spiders/baidu_search.py文件 参考资料 进入baidu_search/baidu_search/ 目录下，执行 1scrapy crawl baidu_search ​]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PyNLPIR使用]]></title>
    <url>%2F2018%2F11%2F26%2FPyNLPIR%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[摘要：pynlpir的安装及简单使用 1 安装PyNLPIR官网地址 1pip install PyNLPIR 2 初始化NLPIR123import pynlpir# 默认情况下，输入假定为unicode或UTF-8编码。如果您想使用不同的编码（例如GBK或BIG5）pynlpir.open(encoding=&apos;gbk&apos;) 3 切分文本1234# 词性标注 pos_tagging=True；词性标注显示英文/中文 pos_english=True； 词性标记的显示方式 pos_names='parent/child/all'# 返回的是tuple(token, pos)组成的列表，其中token就是切出来的词，pos就是语言属性# 调用segment方法指定的pos_names参数可以是'all', 'child', 'parent'，默认是parent， 表示获取该词性的最顶级词性，child表示获取该词性的最具体的信息，all表示获取该词性相关的所有词性信息，相当于从其顶级词性到该词性的一条路径pynlpir.segment(s, pos_tagging=True, pos_names='parent', pos_english=True) 1234s = &apos;NLPIR分词系统前身为2000年发布的ICTCLAS词法分析系统，从2009年开始，为了和以前工作进行大的区隔，并推广NLPIR自然语言处理与信息检索共享平台，调整命名为NLPIR分词系统。&apos;pynlpir.segment(s)# Sample output: [(&apos;NLPIR&apos;, &apos;noun&apos;), (&apos;分词&apos;, &apos;verb&apos;), (&apos;系统&apos;, &apos;noun&apos;), (&apos;前身&apos;, &apos;noun&apos;), (&apos;为&apos;, &apos;preposition&apos;), (&apos;2000年&apos;, &apos;time word&apos;), (&apos;发布&apos;, &apos;verb&apos;), . . . ] 如果不想词性标注，设置post_tagging为false： 123pynlpir.segment(s, pos_tagging=False)# Sample output: ['NLPIR', '分词', '系统', '前身', '为', '2000年', '发布', . . . ] 4 关键字1234# 获得多少个词：max_words=50； 显示关键字权重：weighted=Truepynlpir.get_key_words(s, max_words=5, weighted=True)# 关闭APIpynlpir.close()]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于TextRank自动摘要]]></title>
    <url>%2F2018%2F11%2F23%2F%E5%9F%BA%E4%BA%8ETextRank%E8%87%AA%E5%8A%A8%E6%91%98%E8%A6%81%2F</url>
    <content type="text"><![CDATA[摘要：本文主要讲解了TextRank算法和基于TextRank进行文本自动摘要提取 1 TextRank算法经典的TextRank算法是在 Google公司PageRank算法的启发下，利用投票的原理让每一个节点为它的邻居节点投赞成票，票的权重取决于节点本身的票数 。这是一个“先有蛋还是先有鸡”的悖论。TextRank算法借鉴了PageRank的做法，采用矩阵迭代收敛的方式解决了这个悖论。 TextRank网络图 G=(V,E,W) V : 节点集合。 V={V1,V2, … , Vn} 是由n个元素 Vi (1&lt;=i&lt;=n) 所构成的集合。 E: 边的集合。以 V 中各个 Vi 为节点并以节点的相似关系为边 E=｛(Vi, Vj) | Vi,Vj属于V, w_ij != 0｝ 即边的权重不为0 W: 边上权重的集合。w_ij 是节点 V_i 与 V_j 间边的权重值，通过距离相似度计算函数计算得出（如：欧式距离、Jaccard 或余弦函数等） 相似度矩阵：计算边的权值 该相似度矩阵为对称矩阵，对角线上元素的值为1. 迭代计算公式：迭代计算各个节点的权重 WS(V_i) : 节点 V_i 的权重值（称为PR值） d: 阻尼系数，一般设置为0.85 In(V_i) : 指向V_i 的节点集合 Out(V_i) : V_i 所指向的节点集合 |Out(V_j)| : 集合 Out(V_j) 中元素的个数 上式中左边表示 V_i 的权重，右侧的求和表示每个相邻节点对本节点的贡献程度。求和的分子 w_ij 表示 V_i 和 V_j 间的相似程度，分母为一个加权和，WS(V_j) 代表上一次迭代后节点 V_j 的权重值。 由于计算节点的权重时又需要用到节点本身的权重，因此需要进行迭代。假设每个节点的权重值初始化均为 1/|V|, 即 B_0 = (1/|V|, 1/|V|, … , 1/|V|)T ,经过若干次迭代计算后可收敛。 当两次迭代的结果B_i 和 B_i-1 差别非常小并且接近于0时停止迭代计算。一般经过20-30次迭代就可以达到收敛。 2 文本的 TextRank 网络图构造2.1 文本预处理及特种选择给定一段中文文本，将每个句子作为一个窗口进行分词得到该句子的各个特征项。由于这些特征项的维数非常高，存在许多对摘要提取无用的特征，因此 利用停用词表去掉对这些无用的词，并进行敏感词的过滤 根据 Zipf 法则（也称幂集定律）合理的删除大量低频词 来降低特征空间的维数。 为减少冗余度和提高效果，可以采取特征词相关性分析、聚类、同义词和近义词归并等策略。 预处理之后得到特征词向量 利用相似度函数计算句子之间的相似度，并将其作为句子间边的权重。如果不存在相似度，则他们之间没有边。 给段中文文本D，假定包含n个句子， D={P_1, P_2, … , P_n}, 其中P_i按D 中出现的先后顺序进行排序的句子。于是根据上图的预处理流程对D及P_i进行处理可得： D的特征词向量 每句的特征词向量 用TF-IDF权值法作为特征词的评估函数 （考虑文档长度对权值的影响，并弱化瓷瓶差异较大所带来的影响）评估函数为： N 为分词工具中词典所包含的特征词的总数，N_keyj 为 N_keyj 在 N 中出现的次数。 根据计算结果对特征词进行排序，并取前几个特征词就可得到文档对应的关键词列表。 2.2 TextRank 网络图给定各个句子的特征向量空间，可以通过各种距离相似度计算函数计算句子相似度。 余弦相似度函数： 句子相似度矩阵： w_ij 表示句子 P_i 和 P_j 间的相似度。对称矩阵，对角线为1. 以D 中各句子 P_i 为节点并以句子间的相似关系为边，并以句子间的相似度为边的权值可构成一个无向的加权 TextRank 的网络图，其中各节点的权重计算： ( 7 ) 设每个节点的权重值初始化均为 1/|D|, 即 B_0 = (1/|D, 1/|D|, … , 1/|D|)T ,经过若干次迭代计算后可收敛。 收敛后的 B_i 包含了各个句子节点的权重值，根据值的大小进行倒排序可得到相应的句子重要性排名。再选择一定数量 N ([1, |D|])的句子，并结合它们在文本中的先后顺序进行组织就可以构成文本的摘要。如： 根据语料库统计分析摘要句子数量与文本句子数量间的比例以确定合适的N值，或者直接简单地取文本句子总数的一定比例作为摘要句子的数量。 3 基于改进TextRank算法的自动摘要提取3.1 改进TextRank算法 句子与标题之间的相似度 计算各句子与标题句子间的相似度，相似度越高，权重越高。 若标题与句子的特征词完全相同，即其相似度为1，则该句子的最终权重再放大2倍，若完全不同，则保持原权重。 各句子中的特征词是否在标题中也同时出现。 若出现则提升其词频的权重，繁殖，保持词频权重不变。 特殊段落中的句子位置 根据段落及段落中旬子的位置进行加权，对首段中越靠前的句子给予越大的权重提升，末段中越靠后的句子给予越小的权重提升。考虑到收敛后的权重矩阵 B 仍 按句子的先后顺序排序，因此可根据句子的位置设置相应 的权重调整向量 关键句子的处理 如果一个句子自成一段 ，那么这个段落往往起着“ 承上启下 ” 或者“ 过渡句” 的作用 。另外 ，文章中可能有一些小标题 ，也是自成一段的。这些段落一般具有 高概括性 、精炼性的特点，符合摘要本身的要求，故有更大的可能性作为摘 要的一部分 。 特殊句子权重的传递 对首段句子 、末段句子 、关键句子( 统称为特殊句子 ) 进行标记 ，并放大这些 句子传递出去的权重 ，使与之关联的句子获得更大的权值。 句子长度过滤 过长或过短的句子都不应该作为生成摘要的候选句 。 3.2 算法实现 根据文档 D构造各个句子的特征词向量 P，并进行句子的长度过滤 。 得到特征词词频矩阵 D_n*h’ 得到标题的向量矩阵 P_0 = [ k_01, … , k_0h’ ]T , 并分别计算各句子与标题的相似度 由式 ( 9 ) 得到 向量 TD_n*1 = [ tws_1, … , tws_n ]T 用以调整最后句子权重 由式( 1 0 ) 调整矩阵 D_n*h 用以调整各句子中的词频的权重 根据式 ( 7 ) 及 矩阵 D_n h 计算各句子间的相似度 ， 并得到相似度矩阵SD_nn 根据 SD_n*n 构建基本的 TextRank 网络图 G 特殊段落的处理和特殊句子权重的传递 ，更新相似度矩阵 SD_n*n 更新图 G，并得到新的 TextRank图 G‘ ， 在 G‘ 上运行 iTextRank 算法获得节点的收敛值矩阵 B_i 由式 ( 11 ) 得到转移矩阵 FP_n*1 依次根据 TD_n1 和 FP_n 1 调整矩阵 B_i 输出句子重要度排序结果 P ， 并 根据句子的先后顺序生成最终的摘要 D]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MLE、MAP和贝叶斯估计]]></title>
    <url>%2F2018%2F11%2F20%2FMLE%E3%80%81MAP%E5%92%8C%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1%2F</url>
    <content type="text"></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Hexo + Github Pages搭建个人博客]]></title>
    <url>%2F2018%2F11%2F19%2F%E5%9F%BA%E4%BA%8EHexo-Github-Pages%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[摘要：搭建个人博客 1. 搭建个人博客1.1 安装 Git : 官网下载地址1.2 安装 Node.js : 官网下载地址1234# 打开cmd,以此执行以下步骤，验证是否安装成功。pathnode -vnpm -v 1.3 安装Hexo 在电脑中新建一个 blog 文件夹存放自己的博客，在文件夹内右键点击 Git Bash 进入命令窗口，执行以下代码： 1npm install -g hexo-cli 初始化 Hexo，得到 hexo 文件夹，用于存放 Hexo 博客所有的文件，包括下面会讲到的主题文件，Git Bash 窗口执行以下代码：（无特别提示，以下代码基本都在 Git Bash 命令窗口执行） 配置Hexo, 进入 hexo 文件夹安装依赖，部署形成的文件，分别执行以下代码： 123cd hexonpm installhexo generate 启动服务器：执行以下代码，可以看到服务器端口号是 4000 1hexo server 打开浏览器，地址栏输入http://localhost:4000/ ，结果如下图，可以看到，初始化的 Hexo 博客搭建成功，可以访问。 2. 将初始化的 Hexo 博客部署到 GitHub Pages 注册一个 Github 帐号，新建一个仓库，仓库名为：compassblog.github.io ，如下图所示：（由于我的仓库已经创建，所以会显示仓库已经存在，并且这个仓库的名称必须严格按照 username.github.io 的格式来命名。username就是GitHub登录名） 进入已经建好的仓库，点击 settings ，找到 GitHub Pages 选项，点击 Choose a theme 选择一个主题，然后点击 select theme 选择主题，如下图所示：（到这一步其实已经可以在地址栏访问自己选择的主题了，选择主题这一步其实可以忽略，但我觉得 GitHub 提供的主题还是蛮酷的，所以就附上这一步吧） 配置 Git 个人信息：在 compassblog 目录打开一个 Git Bash 窗口，输入下面的命令 生成 SSH KEY，其实就是生成一个公钥和密钥，因为 GitHub 需要一个密钥才能与本地相连接。执行以下命令，并连续按 3 次回车生成密钥： SSH KEY 生成之后会默认保存在 C:/Users/电脑名用户名/.ssh 目录中，打开这个目录，打开 id_rsa.pub 文件，复制全部内容，即复制密钥。 打开 GitHub ，依次点击 头像–&gt;Settings–&gt;SSH and GPG keys–&gt;New SSH key，将复制的密钥粘贴到 key 输入框，最后点击 Add Key ，SSH KEY 配置成功，如下图所示： 修改 hexo 文件夹下的 _config.yml 全局配置文件，修改 deploy 属性代码，将本地 hexo 项目托管到 GitHub 上，如下图所示： 执行下面的命令，安装 hexo-deployer-git 插件，快速把代码托管到 GitHub 上(在hexo文件夹下，Gitbash里执行) 执行下面的代码命令，将 hexo 项目托管到 GitHub 上 123hexo cleanhexo ghexo d 浏览器地址栏输入 https://username.github.io/ 访问，可以看到博客已经部署到 GitHub 上，正常访问，如下图所示： 3.配置博客Next 使用文档 Hexo主题配置 个人博客设置 与所有 Hexo 主题启用的模式一样。 当 克隆/下载 完成后，打开 站点配置文件， 找到 theme 字段，并将其值更改为 next。 1theme: next 3.1 添加“标签”页面 在 Hexo 目录下，新建一个页面，命名为 tags（Hexo 目录，gitbash下） ： 1$ hexo new page tags 注意：如果有启用 多说 或者 Disqus 评论，页面也会带有评论。 若需要关闭的话，请添加字段 comments 并将值设置为 false，如： 1234title: 标签date: 2014-12-22 12:39:04type: &quot;tags&quot;comments: false 在菜单中添加链接。编辑 主题配置文件 ， 添加 tags 到 menu 中，如下: 1234menu: home: / archives: /archives tags: /tags 3.2 添加“分类”页面 在 Hexo 目录下，新建一个页面，命名为 categories（Hexo 目录，gitbash下） ： 1$ hexo new page categories 注意：如果有启用 多说 或者 Disqus 评论，页面也会带有评论。 若需要关闭的话，请添加字段 comments 并将值设置为 false，如： 1234title: 分类date: 2014-12-22 12:39:04type: &quot;categories&quot;comments: false 在菜单中添加链接。编辑 主题配置文件 ， 添加 tags 到 menu 中，如下: 1234menu: home: / archives: /archives tags: /tags3.3插入本地图片 3.3 插入本地图片 更改站点配置文件 config.yml 1post_asset_folder: true 在hexo 目录中执行 1npm install https://github.com/CodeFalling/hexo-asset-image --save 新建博客，在 post 中会生成一个和博客名相同的文件夹和一个 .md 文件 1hexo new &quot;newblog&quot; 把图片放入文件夹，在 .md 文件中使用 1&#123;% imgurl Github-Pages-Hexo%E4%B8%BB%E9%A2%98%E9%85%8D%E7%BD%AE/newblog/pict.jpg ful-image alt:newblog/pict.jpg %&#125; 3.4 设置阅读全文在首页显示一篇文章的部分内容，并提供一个链接跳转到全文页面是一个常见的需求。 NexT 提供三种方式来控制文章在首页的显示方式。 也就是说，在首页显示文章的摘录并显示 阅读全文 按钮，可以通过以下方法： 在文章中使用 手动进行截断，这是 Hexo 提供的方式，推荐使用。在文章的 front-matter 中添加 description，并提供文章摘录自动形成摘要，在 主题配置文件 中添加： 默认截取的长度为 150 字符，可以根据需要自行设定 123auto_excerpt: enable: true length: 150 3.5 MathJax只讲一种简单的方法 － 插件。安装 1$ npm install hexo-math --save 在 Hexo 文件夹中执行： 1$ hexo math install 在 config.yml 文件中添加： 1plugins: hexo-math 对于不含特殊符号的公式，可以直接使用 MathJax 的 inline math 表达式. 如果含有特殊符号，则需要人肉 escape，如 \ 之类的特殊符号在 LaTex 表达式中出现频率很高，这样就很麻烦，使用 tag 能够省不少事。 具体用法见 Hexo MathJax插件.MathJax用法总结 3.6 设置标题列表 取消段落标题自动编号 进入next主题，修改主题配置文件那里的number为false 12# 路径名为：hexo\themes\next\_config.yml 123456toc: # 目录超长自动换行 enable: true wrap: true # 段落标题自动编号,false为取消自动编号。 number: true 修改目录默认展开 12# 路径：themes/next/source/css/_custom 123456// 文章目录默认展开.post-toc .nav .nav-child &#123; display: block; &#125;//目录字体大小调整.post-toc ol &#123; font-size : 13px; &#125; 4. 在 Hexo 博客发布文章并托管到 GitHub Pages 永远的 Hello Hexo ：在博客目录下的hexo文件夹内右键打开git bash here，在 Git Bash 命令窗口执行下面的命令，新建一篇文章 “Hello Hexo”，到博客目录的 /source/_posts/ 文件夹下可以发现，已经生成了标题为 Hello-Hexo.md 的博客文件，如图所示，我们直接编辑自己的文章即可。 1hexo new &quot;Hello Hexo&quot; 给文章添加分类和标签：直接在所要编辑文章的头部添加如下代码即可 123title: 基于Hexo + Github Pages搭建个人博客date: 2018-11-19 20:51:27tags:[Hexo] 本地启动，浏览器测试预览文章，如图所示： 1hero s 添加阅读全文按钮：在文章的任意位置添加下面命令即可 1&lt;!-- more --&gt; 所要发表的文章在本地预览无误后，在 Git Bash 命令窗口执行以下命令，发布到 GitHub Pages 上 123hexo cleanhexo ghexo d 上传成功后，在浏览器地址栏直接访问自己的域名： 直接访问，即可看到自己编写的文章已经发布到了 GitHub 上。并且每次发布文章到 GitHub 都需要执行下面的流程： 123hexo cleanhexo ghexo d]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F11%2F19%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
