<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[知识图谱构建技术综述]]></title>
    <url>%2F2018%2F12%2F05%2F%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%9E%84%E5%BB%BA%E6%8A%80%E6%9C%AF%E7%BB%BC%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[摘要：知识图谱构建的基本介绍 1 知识图谱的定义与架构1.1 定义知识图谱．是结构化的语义知识库用于以符号形式描述物理世界中的概念及其相互关系．其基本组成单位是实体－关系－实体三元组以及实体及其相关属性－值对实体间通过关系相互联结构成网状的知识结构。 通过知识图谱可以实现Ｗeｂ从网页链接向概念链接转变，支持用户按主题而不是字符串检索，从而真正实现语义检索．基于知识图谱的搜索引擎能够以图形方式向用户反馈结构化的知识，用户不必浏览大量网页就可以准确定位和深度获取知识。 包括三层含义： 知识图谱本身是一个具有属性的实体通过关系链接而成的网状知识库．从图的角度来看知识图谱在本质上是一种概念网络，其中的节点表示物理世界的实体或概念，而实体间的各种语义关系则构成网络中的边．由此知识图谱是对物理世界的一种符号表达。 知识图谱的研究价值在于它是构建在当前Ｗeｂ基础之上的一层覆盖网络，借助知识图谱能够在Ｗeｂ网页之上建立概念间的链接关系，从而以最小的代价将互联网中积累的信息组织起来，成为可以被利用的知识。 知识图谱的应用价值在于它能够改变现有的信息检索方式。一方面通过推理实现概念检索（相对于现有的字符串模糊匹配方式而言）， 另一方面以图形化方式向用户展示经过分类整理的结构化知识，从而使人们从人工过滤网页寻找答案的模式中解脱出来。 1.2 架构知识图谱的架构包括知识图谱自身的逻辑结构以及构建知识图谱所采用的技术（体系）架构，后者是本文讨论的重点。 1.2.1 逻辑架构首先介绍知识图谱的逻辑结构从逻辑上将知识图谱划分为２个层次数据层和模式层。 1.2.1.1 数据层在知识图谱数据层，知识以事实(fact)为单位存储在图数据库．如果以“实体－关系－实体“或者“实体－属性－性值“三元组作为事实的基本表达方式，则存储在图数据库中的所有数据将构成庞大的实体关系网络形成知识的图谱。 1.2.1.2 模式层模式层在数据层之上是知识图谱的核心．在模式层存储的是经过提炼的知识通常采用本体库（有一匹马叫赤兔，那么马这个概念才是本体；有一个美女叫貂蝉，那么美女这个概念才是本体；本体就是知识库本身的存在，和里面的数据没有关系）来管理知识图谱的模式层，借助本体库对公理、规则和约束条件的支持能力来规范实体、关系以及实体的类型和属性等对象之间的联系。本体库在知识图谱中的地位相当于知识库的模具拥有本体库的知识库（知识数据库，包含了知识的本体和知识。Freebase是一个知识库（结构化），维基百科也可以看成一个知识库（半结构化））冗余知识较少。 1.2.2 技术架构 知识图谱有自顶向下和自底向上２种构建方式．所谓自顶向下构建是指借助百科类网站等结构化数据源，从高质量数据中提取本体和模式信息，加入到知识库中；所谓自底向上构建则是借助一定的技术手段从公开采集的数据中提取出资源模式，选择其中置信度较高的新模式经人工审核之后加入到知识库中。 2 知识图谱的构建技术采用自底向上的方式构建知识图谱的过程是一个迭代更新的过程每一轮更新包括３个步骤： 信息抽取 ，即从各种类型的数据源中提取出实体(概念)、 属性以及实体间的相互关系 ，在此基础上形成本体化的知识表达 知识融合 ，在获得新知识之后需要对其进行整合以消除矛盾和歧义 ，比如某些实体可能有多种表达，某个特定称谓也许对应于多个不同的实体等 知识加工 ，对于经过融合的新知识，需要经过质量评估之后（部分需要人工参与甄别）才能将合格的部分加入到知识库中以确保知识库的质量．新增数据之后，可以进行知识推理拓展现有知识得到新知识。 2.1 信息抽取信息抽取是知识图谱构建的第１步，其中的关键问题是如何从异构数据源中自动抽取信息得到候选知识单元．信息抽取是一种自动化地从半结构化和无结构数据中抽取实体、关系以及实体属性等结构化信息的技术。涉及的关键技术包括实体抽取、关系抽取和属性抽取。 2.1.1 实体抽取实体抽取，也称为命名实体识别(named entity recognition, NER) ，是指从文本数据集中自动识别出命名实体．实体抽取的质量（准确率和召回率）对后续的知识获取效率和质量影响极大，因此是信息抽取中最为基础和关键的部分。 2.1.2 关系抽取文本语料经过实体抽取得到的是一系列离散的命名实体，为了得到语义信息还需要从相关语料中提取出实体之间的关联关系，通过关系将实体（概念）联系起来，才能够形成网状的知识结构。研究关系抽取技术的目的就是解决如何从文本语料中抽取实体间的关系这一基本问题。 2.1.3 属性抽取属性抽取的目标是从不同信息源中采集特定实体的属性信息．例如针对某个公众人物，可以从网络公开信息中得到其昵称、生日、国籍、教育背景等信息。属性抽取技术能够从多种数据来源中汇集这些信息，实现对实体属性的完整勾画。 由于可以将实体的属性视为实体与属性值之间的一种名词性关系，因此也可以将属性抽取问题视为关系抽取问题。 2.2 知识融合通过信息抽取，实现了从非结构化和半结构化数据中获取实体、关系以及实体属性信息的目标，然而这些结果中可能包含大量的冗余和错误信息，数据之间的关系也是扁平化的缺乏层次性和逻辑性，因此有必要对其进行清理和整合。知识融合包括２部分内容：实体链接和知识合并。通过知识融合，可以消除概念的歧义，剔除冗余和错误概念，从而确保知识的质量 。 2.2.1 实体链接实体链接（entity linking）是指对于从文本中抽取得到的实体对象，将其链接到知识库中对应的正确实体对象的操作。 实体链接的基本思想是首先根据给定的实体指称项，从知识库中选出一组候选实体对象然后通过相似度计算将指称项链接到正确的实体对象。 实体链接的一般流程是： 从文本中通过实体抽取得到实体指称项 进行实体消歧和共指消解 ，判断知识库中的同名实体与之是否代表不同的含义，以及知识库中是否存在其他命名实体与之表示相同的含义 在确认知识库中对应的正确实体对象之后，将该实体指称项链接到知识库中对应实体 。 2.2.1.1 实体消歧实体消歧（entity disambiguation）是专门用于解决同名实体产生歧义问题的技术．在实际语言环境中经常会遇到某个实体指称项对应于多个命名实体对象的问题。例如李娜这个名词（指称项）可以对应于作为歌手的李娜这个实体，也可以对应于作为网球运动员的李娜这个实体，通过实体消歧就可以根据当前的语境准确建立实体链接．实体消歧主要采用聚类法 。 聚类法是指以实体对象为聚类中心将所有指向同一目标实体对象的指称项聚集到以该对象为中心的类别下。聚类法消歧的关键问题是如何定义实体对象与指称项之间的相似度 ，常用方法有４种。 空间向量模型词袋模型。典型的方法是取当前语料中实体指称项周边的词构成特征向量，然后利用向量的余弦相似度进行比较，将该指称项聚类到与之最相近的实体指称项集合中。 语义模型 。该模型与空间向量模型类似，区别在于特征向量的构造方法不同，语义模型的特征向量不仅包含词袋向量而且包含一部分语义特征。 社会网络模型 。该模型的基本假设是物以类聚、人以群分，在社会化语境中，实体指称项的意义在很大程度上是由与其相关联的实体所决定的．建模时，首先利用实体间的关系将与之相关的指称项链接起来构成网络然，后利用社会网络分析技术计算该网络中节点之间的拓扑距离（网络中的节点即实体的指称项），以此来判定指称项之间的相似度。 百科知识模型．百科类网站通常会为每个实体（指称项）分配一个单独页面，其中包括指向其他实体页面的超链接，百科知识模型正是利用这种链接关系来计算实体指称项之间的相似度。 2.2.1.2 共指消解共指消解（entity resolution） 技术主要用于解决多个指称项对应于同一实体对象的问题。例如在一篇新闻稿中“Barack Obama”, “president Obama”, “the president”等指称项可能指向的是同一实体对象，其中的许多代词如”he”，”him” 等也可能指向该实体对象．利用共指消解技术可以将这些指称项关联(合并)到正确的实体对象．由于该问题在信息检索和自然语言处理等领域具有特殊的重要性，吸引了大量的研究努力，因此学术界对该问题有多种不同的表述典型的包括：对象对齐(object alignment)、 实体匹配(entity matching)、以及实体同义（entity synonyms）。 2.2.2 知识合并在构建知识图谱时，可以从第三方知识库产品或已有结构化数据获取知识输入。 2.2.2.1 合并外部知识库将外部知识库融合到本地知识库需要处理２个层面的问题． 数据层的融合，包括实体的指称、属性、关系以及所属类别等，主要的问题是如何避免实例以及关系的冲突问题，造成不必要的冗余 通过模式层的融合将新得到的本体融入已有的本体库中。 2.2.2.2 合并关系数据库在知识图谱构建过程中一个重要的高质量知识来源是企业或者机构自己的关系数据库。 2.3 知识加工通过信息抽取，可以从原始语料中提取出实体、关系与属性等知识要素．再经过知识融合，可以消除实体指称项与实体对象之间的歧义得到一系列基本的事实表达．然而，事实本身并不等于知识，要想最终获得结构化、网络化的知识体系，还需要经历知识加工的过程．知识加工主要包括３方面内容：本体构建、知识推理和质量评估。 2.3.1 本体构建本体（ontology）是对概念进行建模的规范，是描述客观世界的抽象模型，以形式化方式对概念及其之间的联系给出明确定义．本体的最大特点在于它是共享的，本体中反映的知识是一种明确定义的共识．虽然在不同时代和领域学者们对本体曾经给出过不同的定义，但这些定义的内涵是一致的，即：本体是同一领域内的不同主体之间进行交流的语义基础．本体是树状结构，相邻层次的节点（概念）之间具有严格的“IsA” 关系，这种单纯的关系有助于知识推理，但却不利于表达概念的多样性．在知识图谱中，本体位于模式层，用于描述概念层次体系是知识库中知识的概念模板。 2.3.2 知识推理知识推理是指从知识库中已有的实体关系数据出发，经过计算机推理建立实体间的新关联，从而拓展和丰富知识网络．知识推理是知识图谱构建的重要手段和关键环节，通过知识推理，能够从现有知识中发现新的知识．例如已知（乾隆，父亲，雍正）和（雍正，父亲，康熙）可以得到（乾隆，祖父，康熙）或（康熙，孙子，乾隆）．知识推理的对象并不局限于实体间的关系也可以是实体的属性值、本体的概念层次关系等．例如已知某实体的生日属性，可以通过推理得到该实体的年龄属性．根据本体库中的概念继承关系，也可以进行概念推理，例如已知（老虎，科，猫科）和（猫科，目，食肉目）可以推出（老虎，目，食肉目）。 知识的推理方法可以分为２大类：基于逻辑的推理和基于图的推理 2.3.3 质量评估质量评估也是知识库构建技术的重要组成部分． 受现有技术水平的限制，采用开放域信息抽取技术得到的知识元素有可能存在错误（如实体识别错误、关系抽取错误等）经过知识推理得到的知识的质量同样也是没有保障的，因此在将其加入知识库之前需要有一个质量评估的过程 随着开放关联数据项目的推进，各子项目所产生的知识库产品间的质量差异也在增大，数据间的冲突日益增多，如何对其质量进行评估，对于全局知识图谱的构建起着重要的作用． 引入质量评估的意义在于：可以对知识的可信度进行量化 ，通过舍弃置信度较低的知识，可以保障知识库的质量。 2.4 知识更新人类所拥有的信息和知识量都是时间的单调递增函数，因此知识图谱的内容也需要与时俱进，其构建过程是一个不断迭代更新的过程。 从逻辑上看知识库的更新包括概念层的更新和数据层的更新．概念层的更新是指新增数据后获得了新的概念，需要自动将新的概念添加到知识库的概念层中．数据层的更新主要是新增或更新实体、关系和属性值，对数据层进行更新需要考虑数据源的可靠性，数据的一致性（是否存在矛盾或冗余等问题）等多方面因素．当前流行的方法是选择百科类网站等可靠数据源，并选择在各数据源中出现频率高的事实和属性加入知识库．知识的更新也可以采用众包的模式（如 Freebase），而对于概念层的更新，则需要借助专业团队进行人工审核。 知识图谱的内容更新有２种方式数据驱动下的全面更新和增量更新．所谓全面更新是指以更新后的全部数据为输入，从零开始构建知识图谱．这种方式比较简单，但资源消耗大，而且需要耗费大量人力资源进行系统维护；而增量更新，则是以当前新增数据为输入，向现有知识图谱中添加新增知识．这种方式资源消耗小，但目前仍需要大量人工干预（定义规则等，因此实施起来十分困难。]]></content>
      <categories>
        <category>知识图谱</category>
      </categories>
      <tags>
        <tag>nlp</tag>
        <tag>知识图谱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聊天机器人]]></title>
    <url>%2F2018%2F11%2F26%2F%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA%2F</url>
    <content type="text"><![CDATA[摘要：制作个人聊天机器人笔记 1 聊天机器人简介1.1 工作原理1.1.1 核心模块 意图识别 分类器（SVM，深度学习） 实体识别 NER（命名实体识别） 对话管理系统 回复生成 1.2 关键技术 海量文本知识表示：网络文本资源获取、机器学习方法、大规模语义计算和推理、知识表示体系、知识库构建； 问句解析：中文分词、词性标注、实体标注、概念类别标注、句法分析、语义分析、逻辑结构标注、指代消解、关联关系标注、问句分类（简单问句还是复杂问句、实体型还是段落型还是篇章级问题）、答案类别确定； 答案生成与过滤：候选答案抽取、关系推演（并列关系还是递进关系还是因果关系）、吻合程度判断、噪声过滤 1.3 技术方法 基于检索的技术（淘汰） 基于模式匹配的技术（淘汰） 基于意图识别的⽅法 ⽣成式⽅法（i.e.,端到端） 2 词性标注和关键词提取2.1 安装PyNLPIR官网地址 1pip install PyNLPIR 2.2 初始化NLPIR123import pynlpir# 默认情况下，输入假定为unicode或UTF-8编码。如果您想使用不同的编码（例如GBK或BIG5）pynlpir.open(encoding=&apos;gbk&apos;) 2.3 分词及词性标注1234# 词性标注 pos_tagging=True；词性标注显示英文/中文 pos_english=True； 词性标记的显示方式 pos_names='parent/child/all'# 返回的是tuple(token, pos)组成的列表，其中token就是切出来的词，pos就是语言属性# 调用segment方法指定的pos_names参数可以是'all', 'child', 'parent'，默认是parent， 表示获取该词性的最顶级词性，child表示获取该词性的最具体的信息，all表示获取该词性相关的所有词性信息，相当于从其顶级词性到该词性的一条路径pynlpir.segment(s, pos_tagging=True, pos_names='parent', pos_english=True) 1234s = &apos;NLPIR分词系统前身为2000年发布的ICTCLAS词法分析系统，从2009年开始，为了和以前工作进行大的区隔，并推广NLPIR自然语言处理与信息检索共享平台，调整命名为NLPIR分词系统。&apos;pynlpir.segment(s)# Sample output: [(&apos;NLPIR&apos;, &apos;noun&apos;), (&apos;分词&apos;, &apos;verb&apos;), (&apos;系统&apos;, &apos;noun&apos;), (&apos;前身&apos;, &apos;noun&apos;), (&apos;为&apos;, &apos;preposition&apos;), (&apos;2000年&apos;, &apos;time word&apos;), (&apos;发布&apos;, &apos;verb&apos;), . . . ] 如果不想词性标注，设置post_tagging为false： 123pynlpir.segment(s, pos_tagging=False)# Sample output: ['NLPIR', '分词', '系统', '前身', '为', '2000年', '发布', . . . ] 2.4 关键字提取1234# 获得多少个词：max_words=50； 显示关键字权重：weighted=Truepynlpir.get_key_words(s, max_words=50, weighted=True)# 关闭APIpynlpir.close() 2.5 词性分类表123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116POS_MAP = &#123; &apos;n&apos;: (&apos;名词&apos;, &apos;noun&apos;, &#123; &apos;nr&apos;: (&apos;人名&apos;, &apos;personal name&apos;, &#123; &apos;nr1&apos;: (&apos;汉语姓氏&apos;, &apos;Chinese surname&apos;), &apos;nr2&apos;: (&apos;汉语名字&apos;, &apos;Chinese given name&apos;), &apos;nrj&apos;: (&apos;日语人名&apos;, &apos;Japanese personal name&apos;), &apos;nrf&apos;: (&apos;音译人名&apos;, &apos;transcribed personal name&apos;) &#125;), &apos;ns&apos;: (&apos;地名&apos;, &apos;toponym&apos;, &#123; &apos;nsf&apos;: (&apos;音译地名&apos;, &apos;transcribed toponym&apos;), &#125;), &apos;nt&apos;: (&apos;机构团体名&apos;, &apos;organization/group name&apos;), &apos;nz&apos;: (&apos;其它专名&apos;, &apos;other proper noun&apos;), &apos;nl&apos;: (&apos;名词性惯用语&apos;, &apos;noun phrase&apos;), &apos;ng&apos;: (&apos;名词性语素&apos;, &apos;noun morpheme&apos;), &#125;), &apos;t&apos;: (&apos;时间词&apos;, &apos;time word&apos;, &#123; &apos;tg&apos;: (&apos;时间词性语素&apos;, &apos;time morpheme&apos;), &#125;), &apos;s&apos;: (&apos;处所词&apos;, &apos;locative word&apos;), &apos;f&apos;: (&apos;方位词&apos;, &apos;noun of locality&apos;), &apos;v&apos;: (&apos;动词&apos;, &apos;verb&apos;, &#123; &apos;vd&apos;: (&apos;副动词&apos;, &apos;auxiliary verb&apos;), &apos;vn&apos;: (&apos;名动词&apos;, &apos;noun-verb&apos;), &apos;vshi&apos;: (&apos;动词&quot;是&quot;&apos;, &apos;verb 是&apos;), &apos;vyou&apos;: (&apos;动词&quot;有&quot;&apos;, &apos;verb 有&apos;), &apos;vf&apos;: (&apos;趋向动词&apos;, &apos;directional verb&apos;), &apos;vx&apos;: (&apos;行事动词&apos;, &apos;performative verb&apos;), &apos;vi&apos;: (&apos;不及物动词&apos;, &apos;intransitive verb&apos;), &apos;vl&apos;: (&apos;动词性惯用语&apos;, &apos;verb phrase&apos;), &apos;vg&apos;: (&apos;动词性语素&apos;, &apos;verb morpheme&apos;), &#125;), &apos;a&apos;: (&apos;形容词&apos;, &apos;adjective&apos;, &#123; &apos;ad&apos;: (&apos;副形词&apos;, &apos;auxiliary adjective&apos;), &apos;an&apos;: (&apos;名形词&apos;, &apos;noun-adjective&apos;), &apos;ag&apos;: (&apos;形容词性语素&apos;, &apos;adjective morpheme&apos;), &apos;al&apos;: (&apos;形容词性惯用语&apos;, &apos;adjective phrase&apos;), &#125;), &apos;b&apos;: (&apos;区别词&apos;, &apos;distinguishing word&apos;, &#123; &apos;bl&apos;: (&apos;区别词性惯用语&apos;, &apos;distinguishing phrase&apos;), &#125;), &apos;z&apos;: (&apos;状态词&apos;, &apos;status word&apos;), &apos;r&apos;: (&apos;代词&apos;, &apos;pronoun&apos;, &#123; &apos;rr&apos;: (&apos;人称代词&apos;, &apos;personal pronoun&apos;), &apos;rz&apos;: (&apos;指示代词&apos;, &apos;demonstrative pronoun&apos;, &#123; &apos;rzt&apos;: (&apos;时间指示代词&apos;, &apos;temporal demonstrative pronoun&apos;), &apos;rzs&apos;: (&apos;处所指示代词&apos;, &apos;locative demonstrative pronoun&apos;), &apos;rzv&apos;: (&apos;谓词性指示代词&apos;, &apos;predicate demonstrative pronoun&apos;), &#125;), &apos;ry&apos;: (&apos;疑问代词&apos;, &apos;interrogative pronoun&apos;, &#123; &apos;ryt&apos;: (&apos;时间疑问代词&apos;, &apos;temporal interrogative pronoun&apos;), &apos;rys&apos;: (&apos;处所疑问代词&apos;, &apos;locative interrogative pronoun&apos;), &apos;ryv&apos;: (&apos;谓词性疑问代词&apos;, &apos;predicate interrogative pronoun&apos;), &#125;), &apos;rg&apos;: (&apos;代词性语素&apos;, &apos;pronoun morpheme&apos;), &#125;), &apos;m&apos;: (&apos;数词&apos;, &apos;numeral&apos;, &#123; &apos;mq&apos;: (&apos;数量词&apos;, &apos;numeral-plus-classifier compound&apos;), &#125;), &apos;q&apos;: (&apos;量词&apos;, &apos;classifier&apos;, &#123; &apos;qv&apos;: (&apos;动量词&apos;, &apos;verbal classifier&apos;), &apos;qt&apos;: (&apos;时量词&apos;, &apos;temporal classifier&apos;), &#125;), &apos;d&apos;: (&apos;副词&apos;, &apos;adverb&apos;), &apos;p&apos;: (&apos;介词&apos;, &apos;preposition&apos;, &#123; &apos;pba&apos;: (&apos;介词“把”&apos;, &apos;preposition 把&apos;), &apos;pbei&apos;: (&apos;介词“被”&apos;, &apos;preposition 被&apos;), &#125;), &apos;c&apos;: (&apos;连词&apos;, &apos;conjunction&apos;, &#123; &apos;cc&apos;: (&apos;并列连词&apos;, &apos;coordinating conjunction&apos;), &#125;), &apos;u&apos;: (&apos;助词&apos;, &apos;particle&apos;, &#123; &apos;uzhe&apos;: (&apos;着&apos;, &apos;particle 着&apos;), &apos;ule&apos;: (&apos;了／喽&apos;, &apos;particle 了/喽&apos;), &apos;uguo&apos;: (&apos;过&apos;, &apos;particle 过&apos;), &apos;ude1&apos;: (&apos;的／底&apos;, &apos;particle 的/底&apos;), &apos;ude2&apos;: (&apos;地&apos;, &apos;particle 地&apos;), &apos;ude3&apos;: (&apos;得&apos;, &apos;particle 得&apos;), &apos;usuo&apos;: (&apos;所&apos;, &apos;particle 所&apos;), &apos;udeng&apos;: (&apos;等／等等／云云&apos;, &apos;particle 等/等等/云云&apos;), &apos;uyy&apos;: (&apos;一样／一般／似的／般&apos;, &apos;particle 一样/一般/似的/般&apos;), &apos;udh&apos;: (&apos;的话&apos;, &apos;particle 的话&apos;), &apos;uls&apos;: (&apos;来讲／来说／而言／说来&apos;, &apos;particle 来讲/来说/而言/说来&apos;), &apos;uzhi&apos;: (&apos;之&apos;, &apos;particle 之&apos;), &apos;ulian&apos;: (&apos;连&apos;, &apos;particle 连&apos;), &#125;), &apos;e&apos;: (&apos;叹词&apos;, &apos;interjection&apos;), &apos;y&apos;: (&apos;语气词&apos;, &apos;modal particle&apos;), &apos;o&apos;: (&apos;拟声词&apos;, &apos;onomatopoeia&apos;), &apos;h&apos;: (&apos;前缀&apos;, &apos;prefix&apos;), &apos;k&apos;: (&apos;后缀&apos; &apos;suffix&apos;), &apos;x&apos;: (&apos;字符串&apos;, &apos;string&apos;, &#123; &apos;xe&apos;: (&apos;Email字符串&apos;, &apos;email address&apos;), &apos;xs&apos;: (&apos;微博会话分隔符&apos;, &apos;hashtag&apos;), &apos;xm&apos;: (&apos;表情符合&apos;, &apos;emoticon&apos;), &apos;xu&apos;: (&apos;网址URL&apos;, &apos;URL&apos;), &apos;xx&apos;: (&apos;非语素字&apos;, &apos;non-morpheme character&apos;), &#125;), &apos;w&apos;: (&apos;标点符号&apos;, &apos;punctuation mark&apos;, &#123; &apos;wkz&apos;: (&apos;左括号&apos;, &apos;left parenthesis/bracket&apos;), &apos;wky&apos;: (&apos;右括号&apos;, &apos;right parenthesis/bracket&apos;), &apos;wyz&apos;: (&apos;左引号&apos;, &apos;left quotation mark&apos;), &apos;wyy&apos;: (&apos;右引号&apos;, &apos;right quotation mark&apos;), &apos;wj&apos;: (&apos;句号&apos;, &apos;period&apos;), &apos;ww&apos;: (&apos;问号&apos;, &apos;question mark&apos;), &apos;wt&apos;: (&apos;叹号&apos;, &apos;exclamation mark&apos;), &apos;wd&apos;: (&apos;逗号&apos;, &apos;comma&apos;), &apos;wf&apos;: (&apos;分号&apos;, &apos;semicolon&apos;), &apos;wn&apos;: (&apos;顿号&apos;, &apos;enumeration comma&apos;), &apos;wm&apos;: (&apos;冒号&apos;, &apos;colon&apos;), &apos;ws&apos;: (&apos;省略号&apos;, &apos;ellipsis&apos;), &apos;wp&apos;: (&apos;破折号&apos;, &apos;dash&apos;), &apos;wb&apos;: (&apos;百分号千分号&apos;, &apos;percent/per mille sign&apos;), &apos;wh&apos;: (&apos;单位符号&apos;, &apos;unit of measure sign&apos;), &#125;), &#125; 3 通过爬虫获取语料信息通过上节得到了关键词，想获取预料信息，通过几大搜索引擎的调用接口获取。 3.1 Anaconda安装Scrapy 首先查看anaconda中是否装有scrapy工具包，具体方法如下：Anaconda Prompt / cmd命令中，输入 conda list,查看所有已经安装的工具包及版本号 如果没有发现scrapy，则执行第2步。 输入 conda install -c scrapinghub scrapy ，等待片刻后，提示需要安装的相关工具包 proceed下输入y，回车， 自动进行安装相关的库。 再一次通过conda list 查看，就可以看到scrapy已经在list中了。安装成功。 3.2 创建Scrapy项目 打开Anaconda Prompt，切换到想要创建的目录下面 12345cd D:\WorkSpace\Spider\# 执行下面语句创建scrapy工程scrapy startproject baidu_search# 将会自动生成了baidu_search目录和下面的文件创建baidu_search/baidu_search/spiders/baidu_search.py文件并对其进行配置，做好抓取器。 参考资料 进入baidu_search/baidu_search/ 目录下，执行 1scrapy crawl baidu_search 4 依存句法和语义依存分析4.1 依存句法分析依存句法就是这些成分之间有一种依赖关系。什么是依赖：没有你的话，我存在就是个错误。“北京是中国的首都”，如果没有“首都”，那么“中国的”存在就是个错误，因为“北京是中国的”表达的完全是另外一个意思了。 4.2 语义依存分析“语义”就是说句子的含义，“张三昨天告诉李四一个秘密”，那么语义包括：谁告诉李四秘密的？张三。张三告诉谁一个秘密？李四。张三什么时候告诉的？昨天。张三告诉李四什么？秘密。 4.3 语义依存和依存句法的区别依存句法强调介词、助词等的划分作用，语义依存注重实词之间的逻辑关系。 另外，依存句法随着字面词语变化而不同，语义依存不同字面词语可以表达同一个意思，句法结构不同的句子语义关系可能相同。 4.4 依存句法分析和语义依存分析对聊天机器人有什么意义呢？依存句法分析和语义分析相结合使用，对对方说的话进行依存句法和语义分析后，一方面可以让计算机理解句子的含义，从而匹配到最合适的回答，另外如果有已经存在的依存句法、语义分析结果，还可以通过置信度匹配来实现聊天回答。 4.5 依存句法分析到底是怎么分析的呢？依存句法分析的基本任务是确定句式的句法结构(短语结构)或句子中词汇之间的依存关系。依存句法分析最重要的两棵树： 依存树：子节点依存于父节点 依存投射树：实线表示依存联结关系，位置低的成分依存于位置高的成分，虚线为投射线 4.6 依存关系的五条公理 一个句子中只有一个成分是独立的 其他成分直接依存于某一成分 任何一个成分都不能依存于两个或两个以上的成分 如果A成分直接依存于B成分，而C成分在句子中位于A和B之间，那么C或者直接依存于B，或者直接依存于A和B之间的某一成分 中心成分左右两面的其他成分相互不发生关系 什么地方存在依存关系呢？比如合成词（如：国内）、短语（如：英雄联盟）很多地方都是 4.7 LTP依存关系标记 4.8 那么依存关系是怎么计算出来的呢？是通过机器学习和人工标注来完成的，机器学习依赖人工标注，那么都哪些需要我们做人工标注呢？分词词性、依存树库、语义角色都需要做人工标注，有了这写人工标注之后，就可以做机器学习来分析新的句子的依存句法了 4.9 LTP云平台怎么用？首先注册用户，得到每月免费20G的流量，在http://www.ltp-cloud.com/注册一个账号，注册好后登陆并进入你的dashboard：http://www.ltp-cloud.com/dashboard/在dashboard里还可以查询自己流量使用情况。具体使用方法如下（参考http://www.ltp-cloud.com/document）： 5 语言模型业界目前比较认可而且有效的语言模型是n元语法模型(n-gram model)，它本质上是马尔可夫模型，简单来描述就是：一句话中下一个词的出现和最近n个词有关(包括它自身)。详细解释一下： 如果这里的n=1时，那么最新一个词只和它自己有关，也就是它是独立的，和前面的词没关系，这叫做一元文法 如果这里的n=2时，那么最新一个词和它前面一个词有关，比如前面的词是“我”，那么最新的这个词是“是”的概率比较高，这叫做二元文法，也叫作一阶马尔科夫链 依次类推，工程上n=3用的是最多的，因为n越大约束信息越多，n越小可靠性更高 n元语法模型实际上是一个概率模型，也就是出现一个词的概率是多少，或者一个句子长这个样子的概率是多少。 这就又回到了之前文章里提到的自然语言处理研究的两大方向：基于规则、基于统计。n元语法模型显然是基于统计的方向。 5.1 语言模型的应用这几乎就是自然语言处理的应用了，有：中文分词、机器翻译、拼写纠错、语音识别、音子转换、自动文摘、问答系统、OCR等]]></content>
      <categories>
        <category>聊天机器人</category>
      </categories>
      <tags>
        <tag>nlp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy学习笔记]]></title>
    <url>%2F2018%2F11%2F26%2FScrapy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[摘要：Scrapy 的安装及创建工程 1 Anaconda安装Scrapy 首先查看anaconda中是否装有scrapy工具包，具体方法如下：Anaconda Prompt / cmd命令中，输入 conda list,查看所有已经安装的工具包及版本号 如果没有发现scrapy，则执行第2步。 输入 conda install -c scrapinghub scrapy ，等待片刻后，提示需要安装的相关工具包 proceed下输入y，回车， 自动进行安装相关的库。 再一次通过conda list 查看，就可以看到scrapy已经在list中了。安装成功。 2 创建Scrapy项目 打开Anaconda Prompt，切换到想要创建的目录下面 12345cd D:\WorkSpace\Spider\# 执行下面语句创建scrapy工程scrapy startproject baidu_search# 将会自动生成了baidu_search目录和下面的文件创建baidu_search/baidu_search/spiders/baidu_search.py文件 参考资料 进入baidu_search/baidu_search/ 目录下，执行 1scrapy crawl baidu_search ​]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PyNLPIR使用]]></title>
    <url>%2F2018%2F11%2F26%2FPyNLPIR%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[摘要：pynlpir的安装及简单使用 1 安装PyNLPIR官网地址 1pip install PyNLPIR 2 初始化NLPIR123import pynlpir# 默认情况下，输入假定为unicode或UTF-8编码。如果您想使用不同的编码（例如GBK或BIG5）pynlpir.open(encoding=&apos;gbk&apos;) 3 切分文本1234# 词性标注 pos_tagging=True；词性标注显示英文/中文 pos_english=True； 词性标记的显示方式 pos_names='parent/child/all'# 返回的是tuple(token, pos)组成的列表，其中token就是切出来的词，pos就是语言属性# 调用segment方法指定的pos_names参数可以是'all', 'child', 'parent'，默认是parent， 表示获取该词性的最顶级词性，child表示获取该词性的最具体的信息，all表示获取该词性相关的所有词性信息，相当于从其顶级词性到该词性的一条路径pynlpir.segment(s, pos_tagging=True, pos_names='parent', pos_english=True) 1234s = &apos;NLPIR分词系统前身为2000年发布的ICTCLAS词法分析系统，从2009年开始，为了和以前工作进行大的区隔，并推广NLPIR自然语言处理与信息检索共享平台，调整命名为NLPIR分词系统。&apos;pynlpir.segment(s)# Sample output: [(&apos;NLPIR&apos;, &apos;noun&apos;), (&apos;分词&apos;, &apos;verb&apos;), (&apos;系统&apos;, &apos;noun&apos;), (&apos;前身&apos;, &apos;noun&apos;), (&apos;为&apos;, &apos;preposition&apos;), (&apos;2000年&apos;, &apos;time word&apos;), (&apos;发布&apos;, &apos;verb&apos;), . . . ] 如果不想词性标注，设置post_tagging为false： 123pynlpir.segment(s, pos_tagging=False)# Sample output: ['NLPIR', '分词', '系统', '前身', '为', '2000年', '发布', . . . ] 4 关键字1234# 获得多少个词：max_words=50； 显示关键字权重：weighted=Truepynlpir.get_key_words(s, max_words=5, weighted=True)# 关闭APIpynlpir.close()]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于TextRank自动摘要]]></title>
    <url>%2F2018%2F11%2F23%2F%E5%9F%BA%E4%BA%8ETextRank%E8%87%AA%E5%8A%A8%E6%91%98%E8%A6%81%2F</url>
    <content type="text"><![CDATA[摘要：本文主要讲解了TextRank算法和基于TextRank进行文本自动摘要提取 1 TextRank算法经典的TextRank算法是在 Google公司PageRank算法的启发下，利用投票的原理让每一个节点为它的邻居节点投赞成票，票的权重取决于节点本身的票数 。这是一个“先有蛋还是先有鸡”的悖论。TextRank算法借鉴了PageRank的做法，采用矩阵迭代收敛的方式解决了这个悖论。 TextRank网络图 G=(V,E,W) V : 节点集合。 V={V1,V2, … , Vn} 是由n个元素 Vi (1&lt;=i&lt;=n) 所构成的集合。 E: 边的集合。以 V 中各个 Vi 为节点并以节点的相似关系为边 E=｛(Vi, Vj) | Vi,Vj属于V, w_ij != 0｝ 即边的权重不为0 W: 边上权重的集合。w_ij 是节点 V_i 与 V_j 间边的权重值，通过距离相似度计算函数计算得出（如：欧式距离、Jaccard 或余弦函数等） 相似度矩阵：计算边的权值 该相似度矩阵为对称矩阵，对角线上元素的值为1. 迭代计算公式：迭代计算各个节点的权重 WS(V_i) : 节点 V_i 的权重值（称为PR值） d: 阻尼系数，一般设置为0.85 In(V_i) : 指向V_i 的节点集合 Out(V_i) : V_i 所指向的节点集合 |Out(V_j)| : 集合 Out(V_j) 中元素的个数 上式中左边表示 V_i 的权重，右侧的求和表示每个相邻节点对本节点的贡献程度。求和的分子 w_ij 表示 V_i 和 V_j 间的相似程度，分母为一个加权和，WS(V_j) 代表上一次迭代后节点 V_j 的权重值。 由于计算节点的权重时又需要用到节点本身的权重，因此需要进行迭代。假设每个节点的权重值初始化均为 1/|V|, 即 B_0 = (1/|V|, 1/|V|, … , 1/|V|)T ,经过若干次迭代计算后可收敛。 当两次迭代的结果B_i 和 B_i-1 差别非常小并且接近于0时停止迭代计算。一般经过20-30次迭代就可以达到收敛。 2 文本的 TextRank 网络图构造2.1 文本预处理及特种选择给定一段中文文本，将每个句子作为一个窗口进行分词得到该句子的各个特征项。由于这些特征项的维数非常高，存在许多对摘要提取无用的特征，因此 利用停用词表去掉对这些无用的词，并进行敏感词的过滤 根据 Zipf 法则（也称幂集定律）合理的删除大量低频词 来降低特征空间的维数。 为减少冗余度和提高效果，可以采取特征词相关性分析、聚类、同义词和近义词归并等策略。 预处理之后得到特征词向量 利用相似度函数计算句子之间的相似度，并将其作为句子间边的权重。如果不存在相似度，则他们之间没有边。 给段中文文本D，假定包含n个句子， D={P_1, P_2, … , P_n}, 其中P_i按D 中出现的先后顺序进行排序的句子。于是根据上图的预处理流程对D及P_i进行处理可得： D的特征词向量 每句的特征词向量 用TF-IDF权值法作为特征词的评估函数 （考虑文档长度对权值的影响，并弱化瓷瓶差异较大所带来的影响）评估函数为： N 为分词工具中词典所包含的特征词的总数，N_keyj 为 N_keyj 在 N 中出现的次数。 根据计算结果对特征词进行排序，并取前几个特征词就可得到文档对应的关键词列表。 2.2 TextRank 网络图给定各个句子的特征向量空间，可以通过各种距离相似度计算函数计算句子相似度。 余弦相似度函数： 句子相似度矩阵： w_ij 表示句子 P_i 和 P_j 间的相似度。对称矩阵，对角线为1. 以D 中各句子 P_i 为节点并以句子间的相似关系为边，并以句子间的相似度为边的权值可构成一个无向的加权 TextRank 的网络图，其中各节点的权重计算： ( 7 ) 设每个节点的权重值初始化均为 1/|D|, 即 B_0 = (1/|D, 1/|D|, … , 1/|D|)T ,经过若干次迭代计算后可收敛。 收敛后的 B_i 包含了各个句子节点的权重值，根据值的大小进行倒排序可得到相应的句子重要性排名。再选择一定数量 N ([1, |D|])的句子，并结合它们在文本中的先后顺序进行组织就可以构成文本的摘要。如： 根据语料库统计分析摘要句子数量与文本句子数量间的比例以确定合适的N值，或者直接简单地取文本句子总数的一定比例作为摘要句子的数量。 3 基于改进TextRank算法的自动摘要提取3.1 改进TextRank算法 句子与标题之间的相似度 计算各句子与标题句子间的相似度，相似度越高，权重越高。 若标题与句子的特征词完全相同，即其相似度为1，则该句子的最终权重再放大2倍，若完全不同，则保持原权重。 各句子中的特征词是否在标题中也同时出现。 若出现则提升其词频的权重，繁殖，保持词频权重不变。 特殊段落中的句子位置 根据段落及段落中旬子的位置进行加权，对首段中越靠前的句子给予越大的权重提升，末段中越靠后的句子给予越小的权重提升。考虑到收敛后的权重矩阵 B 仍 按句子的先后顺序排序，因此可根据句子的位置设置相应 的权重调整向量 关键句子的处理 如果一个句子自成一段 ，那么这个段落往往起着“ 承上启下 ” 或者“ 过渡句” 的作用 。另外 ，文章中可能有一些小标题 ，也是自成一段的。这些段落一般具有 高概括性 、精炼性的特点，符合摘要本身的要求，故有更大的可能性作为摘 要的一部分 。 特殊句子权重的传递 对首段句子 、末段句子 、关键句子( 统称为特殊句子 ) 进行标记 ，并放大这些 句子传递出去的权重 ，使与之关联的句子获得更大的权值。 句子长度过滤 过长或过短的句子都不应该作为生成摘要的候选句 。 3.2 算法实现 根据文档 D构造各个句子的特征词向量 P，并进行句子的长度过滤 。 得到特征词词频矩阵 D_n*h’ 得到标题的向量矩阵 P_0 = [ k_01, … , k_0h’ ]T , 并分别计算各句子与标题的相似度 由式 ( 9 ) 得到 向量 TD_n*1 = [ tws_1, … , tws_n ]T 用以调整最后句子权重 由式( 1 0 ) 调整矩阵 D_n*h 用以调整各句子中的词频的权重 根据式 ( 7 ) 及 矩阵 D_n h 计算各句子间的相似度 ， 并得到相似度矩阵SD_nn 根据 SD_n*n 构建基本的 TextRank 网络图 G 特殊段落的处理和特殊句子权重的传递 ，更新相似度矩阵 SD_n*n 更新图 G，并得到新的 TextRank图 G‘ ， 在 G‘ 上运行 iTextRank 算法获得节点的收敛值矩阵 B_i 由式 ( 11 ) 得到转移矩阵 FP_n*1 依次根据 TD_n1 和 FP_n 1 调整矩阵 B_i 输出句子重要度排序结果 P ， 并 根据句子的先后顺序生成最终的摘要 D]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MLE、MAP和贝叶斯估计]]></title>
    <url>%2F2018%2F11%2F20%2FMLE%E3%80%81MAP%E5%92%8C%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1%2F</url>
    <content type="text"></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Hexo + Github Pages搭建个人博客]]></title>
    <url>%2F2018%2F11%2F19%2F%E5%9F%BA%E4%BA%8EHexo-Github-Pages%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[摘要：搭建个人博客 1. 搭建个人博客1.1 安装 Git : 官网下载地址1.2 安装 Node.js : 官网下载地址1234# 打开cmd,以此执行以下步骤，验证是否安装成功。pathnode -vnpm -v 1.3 安装Hexo 在电脑中新建一个 blog 文件夹存放自己的博客，在文件夹内右键点击 Git Bash 进入命令窗口，执行以下代码： 1npm install -g hexo-cli 初始化 Hexo，得到 hexo 文件夹，用于存放 Hexo 博客所有的文件，包括下面会讲到的主题文件，Git Bash 窗口执行以下代码：（无特别提示，以下代码基本都在 Git Bash 命令窗口执行） 配置Hexo, 进入 hexo 文件夹安装依赖，部署形成的文件，分别执行以下代码： 123cd hexonpm installhexo generate 启动服务器：执行以下代码，可以看到服务器端口号是 4000 1hexo server 打开浏览器，地址栏输入http://localhost:4000/ ，结果如下图，可以看到，初始化的 Hexo 博客搭建成功，可以访问。 2. 将初始化的 Hexo 博客部署到 GitHub Pages 注册一个 Github 帐号，新建一个仓库，仓库名为：compassblog.github.io ，如下图所示：（由于我的仓库已经创建，所以会显示仓库已经存在，并且这个仓库的名称必须严格按照 username.github.io 的格式来命名。username就是GitHub登录名） 进入已经建好的仓库，点击 settings ，找到 GitHub Pages 选项，点击 Choose a theme 选择一个主题，然后点击 select theme 选择主题，如下图所示：（到这一步其实已经可以在地址栏访问自己选择的主题了，选择主题这一步其实可以忽略，但我觉得 GitHub 提供的主题还是蛮酷的，所以就附上这一步吧） 配置 Git 个人信息：在 compassblog 目录打开一个 Git Bash 窗口，输入下面的命令 生成 SSH KEY，其实就是生成一个公钥和密钥，因为 GitHub 需要一个密钥才能与本地相连接。执行以下命令，并连续按 3 次回车生成密钥： SSH KEY 生成之后会默认保存在 C:/Users/电脑名用户名/.ssh 目录中，打开这个目录，打开 id_rsa.pub 文件，复制全部内容，即复制密钥。 打开 GitHub ，依次点击 头像–&gt;Settings–&gt;SSH and GPG keys–&gt;New SSH key，将复制的密钥粘贴到 key 输入框，最后点击 Add Key ，SSH KEY 配置成功，如下图所示： 修改 hexo 文件夹下的 _config.yml 全局配置文件，修改 deploy 属性代码，将本地 hexo 项目托管到 GitHub 上，如下图所示： 执行下面的命令，安装 hexo-deployer-git 插件，快速把代码托管到 GitHub 上(在hexo文件夹下，Gitbash里执行) 执行下面的代码命令，将 hexo 项目托管到 GitHub 上 123hexo cleanhexo ghexo d 浏览器地址栏输入 https://username.github.io/ 访问，可以看到博客已经部署到 GitHub 上，正常访问，如下图所示： 3.配置博客Next 使用文档 Hexo主题配置 个人博客设置 与所有 Hexo 主题启用的模式一样。 当 克隆/下载 完成后，打开 站点配置文件， 找到 theme 字段，并将其值更改为 next。 1theme: next 3.1 添加“标签”页面 在 Hexo 目录下，新建一个页面，命名为 tags（Hexo 目录，gitbash下） ： 1$ hexo new page tags 注意：如果有启用 多说 或者 Disqus 评论，页面也会带有评论。 若需要关闭的话，请添加字段 comments 并将值设置为 false，如： 1234title: 标签date: 2014-12-22 12:39:04type: &quot;tags&quot;comments: false 在菜单中添加链接。编辑 主题配置文件 ， 添加 tags 到 menu 中，如下: 1234menu: home: / archives: /archives tags: /tags 3.2 添加“分类”页面 在 Hexo 目录下，新建一个页面，命名为 categories（Hexo 目录，gitbash下） ： 1$ hexo new page categories 注意：如果有启用 多说 或者 Disqus 评论，页面也会带有评论。 若需要关闭的话，请添加字段 comments 并将值设置为 false，如： 1234title: 分类date: 2014-12-22 12:39:04type: &quot;categories&quot;comments: false 在菜单中添加链接。编辑 主题配置文件 ， 添加 tags 到 menu 中，如下: 1234menu: home: / archives: /archives tags: /tags3.3插入本地图片 3.3 插入本地图片 更改站点配置文件 config.yml 1post_asset_folder: true 在hexo 目录中执行 1npm install https://github.com/CodeFalling/hexo-asset-image --save 新建博客，在 post 中会生成一个和博客名相同的文件夹和一个 .md 文件 1hexo new &quot;newblog&quot; 把图片放入文件夹，在 .md 文件中使用 1&#123;% imgurl Github-Pages-Hexo%E4%B8%BB%E9%A2%98%E9%85%8D%E7%BD%AE/newblog/pict.jpg ful-image alt:newblog/pict.jpg %&#125; 3.4 设置阅读全文在首页显示一篇文章的部分内容，并提供一个链接跳转到全文页面是一个常见的需求。 NexT 提供三种方式来控制文章在首页的显示方式。 也就是说，在首页显示文章的摘录并显示 阅读全文 按钮，可以通过以下方法： 在文章中使用 手动进行截断，这是 Hexo 提供的方式，推荐使用。在文章的 front-matter 中添加 description，并提供文章摘录自动形成摘要，在 主题配置文件 中添加： 默认截取的长度为 150 字符，可以根据需要自行设定 123auto_excerpt: enable: true length: 150 3.5 MathJax只讲一种简单的方法 － 插件。安装 1$ npm install hexo-math --save 在 Hexo 文件夹中执行： 1$ hexo math install 在 config.yml 文件中添加： 1plugins: hexo-math 对于不含特殊符号的公式，可以直接使用 MathJax 的 inline math 表达式. 如果含有特殊符号，则需要人肉 escape，如 \ 之类的特殊符号在 LaTex 表达式中出现频率很高，这样就很麻烦，使用 tag 能够省不少事。 具体用法见 Hexo MathJax插件.MathJax用法总结 4. 在 Hexo 博客发布文章并托管到 GitHub Pages 永远的 Hello Hexo ：在 Git Bash 命令窗口执行下面的命令，新建一篇文章 “Hello Hexo”，到博客目录的 /source/_posts/ 文件夹下可以发现，已经生成了标题为 Hello-Hexo.md 的博客文件，如图所示，我们直接编辑自己的文章即可。 1hexo new &quot;Hello Hexo&quot; 给文章添加分类和标签：直接在所要编辑文章的头部添加如下代码即可 123title: 基于Hexo + Github Pages搭建个人博客date: 2018-11-19 20:51:27tags:[Hexo] 本地启动，浏览器测试预览文章，如图所示： 1hero s 添加阅读全文按钮：在文章的任意位置添加下面命令即可 1&lt;!-- more --&gt; 所要发表的文章在本地预览无误后，在 Git Bash 命令窗口执行以下命令，发布到 GitHub Pages 上 123hexo cleanhexo ghexo d 上传成功后，在浏览器地址栏直接访问自己的域名： 直接访问，即可看到自己编写的文章已经发布到了 GitHub 上。并且每次发布文章到 GitHub 都需要执行下面的流程： 123hexo cleanhexo ghexo d]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F11%2F19%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
